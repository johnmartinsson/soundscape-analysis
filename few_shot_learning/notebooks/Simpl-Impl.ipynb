{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import abc\n",
    "import csv\n",
    "import yaml\n",
    "import h5py\n",
    "import librosa\n",
    "import os\n",
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from glob import glob\n",
    "from itertools import chain\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAs of now most of the code in this notebook is more or less copied from the DCASE repository.\\nMinor changes have been done and more is coming to accomodate more flexible FS learning such as\\nactive episodic training and most likely more stuff.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "As of now most of the code in this notebook is more or less copied from the DCASE repository.\n",
    "Minor changes have been done and more is coming to accomodate more flexible FS learning such as\n",
    "active episodic training and most likely more stuff.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHow to make the framework flexible enough that one can point to which samples in a batch are meant to be\\nsupport/query per class? The implementation in DCASE2021 does not handle this.\\n\\nCurrently return the pcen transposed. Where to transpose it back?\\nBatcher? Most important thing is just to not forget i think.\\nThis doesnt really matter as of now since the model dont care. (Time insensitive)\\n\\nThe code only allows one positive class per segment for now I think.\\nThis might be something we would like to fix? (How?)\\nBinary applications not uninterestig though\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "How to make the framework flexible enough that one can point to which samples in a batch are meant to be\n",
    "support/query per class? The implementation in DCASE2021 does not handle this.\n",
    "\n",
    "Currently return the pcen transposed. Where to transpose it back?\n",
    "Batcher? Most important thing is just to not forget i think.\n",
    "This doesnt really matter as of now since the model dont care. (Time insensitive)\n",
    "\n",
    "The code only allows one positive class per segment for now I think.\n",
    "This might be something we would like to fix? (How?)\n",
    "Binary applications not uninterestig though\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototypical net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE2021\n",
    "\n",
    "def conv_block(in_channels,out_channels):\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels,out_channels,3,padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE2021\n",
    "\n",
    "#TODO introduce parametrization of conv blocks?\n",
    "class Protonet(nn.Module):\n",
    "    def __init__(self, raw_transformer=None):\n",
    "        super(Protonet,self).__init__()\n",
    "        self.raw_transformer = raw_transformer\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv_block(1,128),\n",
    "            conv_block(128,128),\n",
    "            conv_block(128,128),\n",
    "            conv_block(128,128)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        #Is there risk for this to be super slow?\n",
    "        #A naive approach might transform the same data more than once?\n",
    "        #Lookup tables?\n",
    "        if self.raw_transformer is not None:\n",
    "            x = self.raw_transformer.rtoi_standard(x)\n",
    "        (num_samples,seq_len,mel_bins) = x.shape\n",
    "        x = x.view(-1,1,seq_len,mel_bins)\n",
    "        x = self.encoder(x)\n",
    "        return x.view(x.size(0),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Will most likely lean heavily on the implementation of the DCASE2021 task 5 baseline implementation.\n",
    "\n",
    "'''\n",
    "def prototypical_loss(input, target, n_support, supp_idxs=None):\n",
    "    \n",
    "    target_cpu = target.to('cpu')\n",
    "    input_cpu = input.to('cpu')\n",
    "    classes = torch.unique(target_cpu)\n",
    "    n_classes = len(classes)\n",
    "    n_query = target.eq(classes[0].item()).sum().item() - n_support\n",
    "    if supp_idxs is None:\n",
    "        #Rewrite, need to select only n_support. We might have n_query > n_support\n",
    "        supp_idxs = list(map(lambda c: target_cpu.eq(c).nonzero()[:n_support].squeeze(1), classes))\n",
    "        q_idxs = torch.stack(list(map(lambda c: target_cpu.eq(c).nonzero()[n_support:], classes))).view(-1)\n",
    "    else:\n",
    "        #Work from supp_idxs.\n",
    "        q_idxs = None\n",
    "        \n",
    "    prototypes = torch.stack([input_cpu[idx_list].mean(0) for idx_list in supp_idxs])\n",
    "    query_samples = input_cpu[q_idxs]\n",
    "    #I think prototypes has the wrong dimension here?\n",
    "    #Query samples shape (10,1024)\n",
    "    #Prototypes (2,1,1024)\n",
    "    dists = euclidean_dist(query_samples, prototypes)\n",
    "    \n",
    "    #Check\n",
    "    log_p_y = F.log_softmax(-dists, dim=1).view(n_classes, n_query, -1)\n",
    "    target_inds = torch.arange(0, n_classes)\n",
    "    target_inds = target_inds.view(n_classes, 1, 1)\n",
    "    target_inds = target_inds.expand(n_classes, n_query, 1).long()\n",
    "    #.mean() -> 1/NcNq\n",
    "    loss_val = -log_p_y.gather(2, target_inds).squeeze().view(-1).mean()\n",
    "    _, y_hat = log_p_y.max(2)\n",
    "    acc_val = y_hat.eq(target_inds.squeeze()).float().mean()\n",
    "    return loss_val, acc_val\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    * Design choice: Handle most of pre-processing as part of the model (torchlibrosa)?\\n      May ultimately lead to simpler augmentation etc down the line. Work with raw audio as far as possible?\\n      \\n    * Make use of h5py library for storing training, validation and test sets?\\n      Still raw audio sets?\\n    \\n    * Incorporate pytorch Dataloader, seems prudent and a good design choice.\\n      read(h5py) file + Episodic sampler -> Dataloader?\\n      \\n    * Slight change of mind. Datagen and FeatureExtractor is not really worth spending time on for now.\\n      Sure they could be interfaces for a framework up the road but can do without for now since the loop\\n      will most likely be quite task dependent for now.\\n      \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    * Design choice: Handle most of pre-processing as part of the model (torchlibrosa)?\n",
    "      May ultimately lead to simpler augmentation etc down the line. Work with raw audio as far as possible?\n",
    "      \n",
    "    * Make use of h5py library for storing training, validation and test sets?\n",
    "      Still raw audio sets?\n",
    "    \n",
    "    * Incorporate pytorch Dataloader, seems prudent and a good design choice.\n",
    "      read(h5py) file + Episodic sampler -> Dataloader?\n",
    "      \n",
    "    * Slight change of mind. Datagen and FeatureExtractor is not really worth spending time on for now.\n",
    "      Sure they could be interfaces for a framework up the road but can do without for now since the loop\n",
    "      will most likely be quite task dependent for now.\n",
    "      \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Possibly take a h5 file as input and return X_train, Y_train, X_val, Y_val\n",
    "Is this an approach that we like? Is it commonly used for deep learning?\n",
    "'''\n",
    "\n",
    "#DCASE\n",
    "\n",
    "class Datagen():\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        if config.features.raw:\n",
    "            #These obviosly requires more processing down the pipe but that is application dependent.\n",
    "            #Leave be for now\n",
    "            hf = h5py.File(os.path.join(config.path.train_w, 'raw_train.h5'))\n",
    "        else:\n",
    "            hf = h5py.File(os.path.join(config.path.train_w, 'mel_train.h5'))\n",
    "            self.x = hf['features'][:]\n",
    "            self.labels = [s.decode() for s in hf['labels'][:]]\n",
    "            if config.datagen.ltoi:\n",
    "                self.y = class_to_int(self.labels)\n",
    "            else:\n",
    "                self.y = self.labels\n",
    "            if config.datagen.balance:\n",
    "                self.x, self.y = balance_class_distribution(self.x, self.y)\n",
    "            \n",
    "            array_train = np.arange(len(self.x))\n",
    "            if config.datagen.stratify:\n",
    "                _,_,_,_,train_array,valid_array = train_test_split(self.x, self.y, array_train, \\\n",
    "                                                    random_state=config.datagen.random_state, stratify=self.y)\n",
    "            else:\n",
    "                _,_,_,_,train_array,valid_array = train_test_split(self.x, self.y, array_train, \\\n",
    "                                                    random_state=config.datagen.random_state)\n",
    "                \n",
    "            self.train_index = train_array\n",
    "            self.valid_index = valid_array\n",
    "            if config.datagen.normalize:\n",
    "                self.mean, self.std = norm_params(self.x[train_array])\n",
    "            else:\n",
    "                self.mean = None\n",
    "                self.std = None\n",
    "                \n",
    "    def feature_scale(self, x):\n",
    "        return (x - self.mean)/self.std\n",
    "    \n",
    "    def generate_train(self):\n",
    "        train_array = sorted(self.train_index)\n",
    "        valid_array = sorted(self.valid_index)\n",
    "        X_train = self.x[train_array]\n",
    "        Y_train = self.y[train_array]\n",
    "        X_val = self.x[valid_array]\n",
    "        Y_val = self.y[valid_array]\n",
    "        if self.config.datagen.normalize:\n",
    "            X_train = self.feature_scale(X_train)\n",
    "            X_val = self.feature_scale(X_val)\n",
    "        return X_train, Y_train, X_val, Y_val\n",
    "        \n",
    "\n",
    "#In comparison to parent class instances will work on one particular hfile\n",
    "#and return the relevant datasets, pos, neg, query\n",
    "class TestDatagen(Datagen):\n",
    "    \n",
    "    def __init__(self, hfile, config):\n",
    "        \n",
    "        #Debatable if this should be rewritten in the case where we do not normalize.\n",
    "        #Should really give this some thought overall actually?\n",
    "        #Isnt this normalization somewhat weird?\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.hfile = hfile\n",
    "        \n",
    "    def generate_eval(self):\n",
    "        \n",
    "        X_pos = self.hfile['feat_pos'][:]\n",
    "        X_neg = self.hfile['feat_neg'][:]\n",
    "        X_query = self.hfile['feat_query'][:]\n",
    "        if self.config.datagen.normalize:\n",
    "            X_pos = self.feature_scale(X_pos)\n",
    "            X_neg = self.feature_scale(X_neg)\n",
    "            X_query = self.feature_scale(X_query)\n",
    "            \n",
    "        return X_pos, X_neg, X_query\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This could be an interface / abstract class to build audio \n",
    "to some other format instance to plug into feature extractor\n",
    "'''\n",
    "\n",
    "class Spectralizer():\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "        self.sr = config.features.sr\n",
    "        self.n_fft = config.features.n_fft\n",
    "        self.hop = config.features.hop_mel\n",
    "        self.n_mels = config.features.n_mels\n",
    "        self.fmax = config.features.fmax\n",
    "        \n",
    "\n",
    "    def raw_to_spec(self, audio, config):\n",
    "\n",
    "        #Supposedly suggested by librosa.\n",
    "        audio = audio * (2**32)\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(audio, sr=self.sr, n_fft=self.n_fft, hop_length=self.hop,\n",
    "                                                 n_mels=self.n_mels, fmax=self.fmax)\n",
    "\n",
    "        pcen = librosa.core.pcen(mel_spec, sr=self.sr)\n",
    "        pcen = pcen.astype(np.float32)\n",
    "        \n",
    "        #Note that we transform the features here and therefor have time/frame along dim 0.\n",
    "        #Transform back when loading data? Smaksak\n",
    "        return pcen.T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Possibly work on an raw files and annotations and return/write h5 files.\n",
    "This might be clunky to include in a framework since this most likely is dataset dependent.\n",
    "Might however benfit from having an interface which is inherited by classes working on specific datasets.\n",
    "'''\n",
    "\n",
    "class FeatureExtractor(abc.ABC):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "'''\n",
    "Takes the data from the DCASE (all files one folder) and returns h5 file with the datasets 'features' and 'labels'\n",
    "This takes no heed to unlabeled segments and therefor we will have no unlabeled data to work with.\n",
    "This is an interesting TODO. Most likely need to rework some of the mechanisms here to work with limited RAM.\n",
    "Extract segment -> write to file etc... Look at DCASE code for example\n",
    "Unlabeled data could be saved to a new dataset 'unlabeled' for example.\n",
    "\n",
    "\n",
    "TODO: MemError already present even before processing unlabeled data and only one of the smaller audio files.\n",
    "Atleast for the non raw data. Need to fix this. Probably not hard for data processed into spectrograms since\n",
    "we beforehand know the dimensions. Harder for raw audio segments.\n",
    "\n",
    "Why are we getting MemError though? Could run the DCASE program from home with 16GB RAM.\n",
    "Does not load all features into memory at once? Wonky h5py thing? Check this out!\n",
    "\n",
    "It seems the DCASE code loads all the features into memory.\n",
    "\n",
    "Found a bug, this however does not nessecarily discard the above comments.\n",
    "Working well with memory is still most likely of importance when extracting from large sets.\n",
    "'''\n",
    "class MyF_Ext(FeatureExtractor):\n",
    "    \n",
    "    def __init__(self, config, spectralizer=None):\n",
    "        self.config = config\n",
    "        self.spectralizer = spectralizer\n",
    "        \n",
    "    def extract_features(self):\n",
    "        \n",
    "        self.extract_train()\n",
    "        self.extract_test()\n",
    "    \n",
    "    '''\n",
    "    Assumes all *.csv and *.wav files are in the same folder which path is in config.\n",
    "    Either creates spectrograms as features or raw audio segments containing events.\n",
    "    Assumes annotations as those provided in \n",
    "    '''\n",
    "    \n",
    "    def extract_train(self):\n",
    "        \n",
    "        print('--- Processing training data ---')\n",
    "        csv_files = [file for file in glob(os.path.join(self.config.path.data_train, '*.csv'))]\n",
    "        \n",
    "        if self.config.features.raw:\n",
    "            \n",
    "            print('Raw extraction')\n",
    "            \n",
    "            events = []\n",
    "            labels = []\n",
    "            \n",
    "            for file in csv_files:\n",
    "            \n",
    "                print('Processing ' + file.replace('csv', 'wav'))\n",
    "                audio, sr = librosa.load(file.replace('csv', 'wav'), self.config.features.sr)\n",
    "                df = pd.read_csv(file, header=0, index_col=False)\n",
    "                df_pos = df[(df == 'POS').any(axis=1)]\n",
    "                \n",
    "                #Add config options for window size around event\n",
    "                df_pos.loc[:, 'Starttime'] = df_pos['Starttime'] - 0.025\n",
    "                df_pos.loc[:, 'Endtime'] = df_pos['Endtime'] + 0.025\n",
    "                start_time = [int(np.floor(start * sr)) for start in df_pos['Starttime']]\n",
    "                end_time = [int(np.floor(end * sr)) for end in df_pos['Endtime']]\n",
    "                \n",
    "                #Better way of doing this?\n",
    "                for i in range(len(start_time)):\n",
    "                    events += [audio[start_time[i]:end_time[i]]]\n",
    "                    \n",
    "                labels += list(chain.from_iterable(\n",
    "                    [df_pos.columns[(df_pos == 'POS').loc[index]].values for index, _ in df_pos.iterrows()]))\n",
    "            \n",
    "            print('Padding')\n",
    "            #Pad arrays in events and format for write\n",
    "            max_len = 0\n",
    "            for e in events:\n",
    "                if len(e) > max_len:\n",
    "                    max_len = len(e)\n",
    "                    \n",
    "            for i in range(len(events)):\n",
    "                if len(events[i]) < max_len:\n",
    "                    events[i] = np.append(events[i], np.array([self.config.features.raw_pad]*(max_len-len(events[i]))))\n",
    "            \n",
    "            events = np.array(events)\n",
    "            \n",
    "            print('Writing to file')\n",
    "            \n",
    "            hf = h5py.File(os.path.join(self.config.path.train_w, 'raw_train.h5'), 'w')\n",
    "            hf.create_dataset('features', data=events)\n",
    "            hf.create_dataset('labels', data=[s.encode() for s in labels], dtype='S20')\n",
    "            hf.close()\n",
    "            \n",
    "            print('Done')\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #DCASE more or less\n",
    "            \n",
    "            print('Spectrogram extraction')\n",
    "            \n",
    "            fps = self.config.features.sr / self.config.features.hop_mel\n",
    "            seg_len = int(round(self.config.features.seg_len * fps))\n",
    "            hop_seg = int(round(self.config.features.hop_seg * fps))\n",
    "            \n",
    "            labels = []\n",
    "            events = []\n",
    "            \n",
    "            for file in csv_files:\n",
    "                \n",
    "                print('Processing ' + file.replace('csv', 'wav'))\n",
    "                audio, sr = librosa.load(file.replace('csv', 'wav'), self.config.features.sr)\n",
    "                \n",
    "                print('Spectral transform')\n",
    "                pcen = self.spectralizer.raw_to_spec(audio, self.config)\n",
    "                \n",
    "                df = pd.read_csv(file, header=0, index_col=False)\n",
    "                df_pos = df[(df == 'POS').any(axis=1)]\n",
    "                \n",
    "                start_time, end_time = time_2_frame(df_pos, fps)\n",
    "                label_f = list(chain.from_iterable(\n",
    "                    [df_pos.columns[(df_pos == 'POS').loc[index]].values for index, _ in df_pos.iterrows()]))\n",
    "                \n",
    "                print('Slicing spectrogram')\n",
    "                \n",
    "                for index in range(len(start_time)):\n",
    "                    \n",
    "                    str_ind = start_time[index]\n",
    "                    end_ind = end_time[index]\n",
    "                    label = label_f[index]\n",
    "                    \n",
    "                    #Event longer than a segment?\n",
    "                    if end_ind - str_ind > seg_len:\n",
    "                        shift = 0\n",
    "                        while end_ind - (str_ind + shift) > seg_len:\n",
    "                            \n",
    "                            pcen_patch = pcen[int(str_ind + shift):int(str_ind + shift + seg_len)]\n",
    "                            events += [pcen_patch]\n",
    "                            labels.append(label)\n",
    "                            shift += hop_seg\n",
    "                        \n",
    "                        pcen_patch = pcen[end_ind - seg_len:end_ind]\n",
    "                        events += [pcen_patch]\n",
    "                        labels.append(label)\n",
    "                    \n",
    "                    #Event shorter than a segment!\n",
    "                    else:\n",
    "                        \n",
    "                        #Repeat the patch til segment length.\n",
    "                        pcen_patch = pcen[str_ind:end_ind]\n",
    "                        if pcen_patch.shape[0] == 0:\n",
    "                            continue\n",
    "                        \n",
    "                        repeats = int(seg_len/(pcen_patch.shape[0])) + 1\n",
    "                        pcen_patch_new = np.tile(pcen_patch, (repeats, 1))\n",
    "                        pcen_patch_new = pcen_patch_new[0:int(seg_len)]\n",
    "                        events += [pcen_patch_new]\n",
    "                        labels.append(label)\n",
    "                        \n",
    "            print('Writing to file')\n",
    "            \n",
    "            events = np.array(events)\n",
    "            \n",
    "            hf = h5py.File(os.path.join(self.config.path.train_w, 'mel_train.h5'), 'w')\n",
    "            hf.create_dataset('features', data=events)\n",
    "            hf.create_dataset('labels', data=[s.encode() for s in labels], dtype='S20')\n",
    "            hf.close()\n",
    "            \n",
    "            print('Done')\n",
    "                        \n",
    "                \n",
    "                        \n",
    "                \n",
    "    #Try to start out in a way that would make it easier to possibly incorporate multiple negative classes\n",
    "    #down the line. This needs to be reflected in TestDatagen. Perhaps just list of lists with indexes to \n",
    "    #the h5 dataset 'feat_neg'. Then if this is not in keys just assume that only one negative class exists.\n",
    "    \n",
    "    #For now just a copy of the DCASE code, this since the way they work here most likely have an impact on the\n",
    "    #scoring/evaluation metrics on the github.\n",
    "    def extract_test(self):\n",
    "        \n",
    "        print('--- Processing test data ---')\n",
    "        csv_files = [file for file in glob(os.path.join(self.config.path.data_test, '*.csv'))]\n",
    "        \n",
    "        #Are we ever interested in a raw extraction here?\n",
    "        if self.config.features.raw:\n",
    "            pass\n",
    "        else:\n",
    "            \n",
    "            fps = self.config.features.sr / self.config.features.hop_mel\n",
    "            seg_len = int(round(self.config.features.seg_len * fps))\n",
    "            hop_seg = int(round(self.config.features.hop_seg * fps))\n",
    "            \n",
    "            for file in csv_files:\n",
    "                \n",
    "                print('Processing ' + file.replace('csv', 'wav'))\n",
    "                \n",
    "                idx_pos = 0\n",
    "                idx_neg = 0\n",
    "                start_neg = 0\n",
    "                hop_neg = 0\n",
    "                idx_query = 0\n",
    "                hop_query = 0\n",
    "                strt_index = 0\n",
    "\n",
    "                split_list = file.split('/')\n",
    "                name = str(split_list[-1].split('.')[0])\n",
    "                feat_name = name + '.h5'\n",
    "                audio_path = file.replace('csv', 'wav')\n",
    "                feat_info = []\n",
    "                hdf_eval = os.path.join(self.config.path.test_w ,feat_name)\n",
    "                hf = h5py.File(hdf_eval,'w')\n",
    "                hf.create_dataset('feat_pos', shape=(0, seg_len, self.config.features.n_mels),\n",
    "                                  maxshape= (None, seg_len, self.config.features.n_mels))\n",
    "                hf.create_dataset('feat_query',shape=(0,seg_len, self.config.features.n_mels),maxshape=(None,seg_len,self.config.features.n_mels))\n",
    "                hf.create_dataset('feat_neg',shape=(0,seg_len, self.config.features.n_mels),maxshape=(None,seg_len,self.config.features.n_mels))\n",
    "                hf.create_dataset('start_index_query',shape=(1,),maxshape=(None))\n",
    "\n",
    "                'In case you want to use the statistics of each file to normalize'\n",
    "\n",
    "                hf.create_dataset('mean_global',shape=(1,), maxshape=(None))\n",
    "                hf.create_dataset('std_dev_global',shape=(1,), maxshape=(None))\n",
    "\n",
    "                df_eval = pd.read_csv(file, header=0, index_col=False)\n",
    "                Q_list = df_eval['Q'].to_numpy()\n",
    "\n",
    "                start_time,end_time = time_2_frame(df_eval,fps)\n",
    "\n",
    "                index_sup = np.where(Q_list == 'POS')[0][:self.config.train.n_shot]\n",
    "\n",
    "                audio, sr = librosa.load(file.replace('csv', 'wav'), self.config.features.sr)\n",
    "                print('Spectral transform')\n",
    "                pcen = self.spectralizer.raw_to_spec(audio, self.config)\n",
    "               \n",
    "                mean = np.mean(pcen)\n",
    "                std = np.mean(pcen)\n",
    "                hf['mean_global'][:] = mean\n",
    "                hf['std_dev_global'][:] = std\n",
    "\n",
    "                strt_indx_query = end_time[index_sup[-1]]\n",
    "                end_idx_neg = pcen.shape[0] - 1\n",
    "                hf['start_index_query'][:] = strt_indx_query\n",
    "\n",
    "                print(\"Creating negative dataset\")\n",
    "\n",
    "                while end_idx_neg - (strt_index + hop_neg) > seg_len:\n",
    "\n",
    "                    patch_neg = pcen[int(strt_index + hop_neg):int(strt_index + hop_neg + seg_len)]\n",
    "\n",
    "                    hf['feat_neg'].resize((idx_neg + 1, patch_neg.shape[0], patch_neg.shape[1]))\n",
    "                    hf['feat_neg'][idx_neg] = patch_neg\n",
    "                    idx_neg += 1\n",
    "                    hop_neg += hop_seg\n",
    "\n",
    "                last_patch = pcen[end_idx_neg - seg_len:end_idx_neg]\n",
    "                hf['feat_neg'].resize((idx_neg + 1, last_patch.shape[0], last_patch.shape[1]))\n",
    "                hf['feat_neg'][idx_neg] = last_patch\n",
    "\n",
    "                print(\"Creating Positive dataset\")\n",
    "                for index in index_sup:\n",
    "\n",
    "                    str_ind = int(start_time[index])\n",
    "                    end_ind = int(end_time[index])\n",
    "\n",
    "                    if end_ind - str_ind > seg_len:\n",
    "\n",
    "                        shift = 0\n",
    "                        while end_ind - (str_ind + shift) > seg_len:\n",
    "\n",
    "                            patch_pos = pcen[int(str_ind + shift):int(str_ind + shift + seg_len)]\n",
    "\n",
    "                            hf['feat_pos'].resize((idx_pos + 1, patch_pos.shape[0], patch_pos.shape[1]))\n",
    "                            hf['feat_pos'][idx_pos] = patch_pos\n",
    "                            idx_pos += 1\n",
    "                            shift += hop_seg\n",
    "                        last_patch_pos = pcen[end_ind - seg_len:end_ind]\n",
    "                        hf['feat_pos'].resize((idx_pos + 1, patch_pos.shape[0], patch_pos.shape[1]))\n",
    "                        hf['feat_pos'][idx_pos] = last_patch_pos\n",
    "                        idx_pos += 1\n",
    "\n",
    "                    else:\n",
    "                        patch_pos = pcen[str_ind:end_ind]\n",
    "\n",
    "                        if patch_pos.shape[0] == 0:\n",
    "                            print(patch_pos.shape[0])\n",
    "                            print(\"The patch is of 0 length\")\n",
    "                            continue\n",
    "                        repeat_num = int(seg_len / (patch_pos.shape[0])) + 1\n",
    "\n",
    "                        patch_new = np.tile(patch_pos, (repeat_num, 1))\n",
    "                        patch_new = patch_new[0:int(seg_len)]\n",
    "                        hf['feat_pos'].resize((idx_pos + 1, patch_new.shape[0], patch_new.shape[1]))\n",
    "                        hf['feat_pos'][idx_pos] = patch_new\n",
    "                        idx_pos += 1\n",
    "\n",
    "\n",
    "\n",
    "                print(\"Creating query dataset\")\n",
    "\n",
    "                while end_idx_neg - (strt_indx_query + hop_query) > seg_len:\n",
    "\n",
    "                    patch_query = pcen[int(strt_indx_query + hop_query):int(strt_indx_query + hop_query + seg_len)]\n",
    "                    hf['feat_query'].resize((idx_query + 1, patch_query.shape[0], patch_query.shape[1]))\n",
    "                    hf['feat_query'][idx_query] = patch_query\n",
    "                    idx_query += 1\n",
    "                    hop_query += hop_seg\n",
    "\n",
    "\n",
    "                last_patch_query = pcen[end_idx_neg - seg_len:end_idx_neg]\n",
    "\n",
    "                hf['feat_query'].resize((idx_query + 1, last_patch_query.shape[0], last_patch_query.shape[1]))\n",
    "                hf['feat_query'][idx_query] = last_patch_query\n",
    "\n",
    "                hf.close()\n",
    "\n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instance with torchlibrosa to be included in model if input is raw.\n",
    "#Having seconds thaughts on putting the data raw into the models.\n",
    "#Extracting raw features and DataGenning them still of interest.\n",
    "#But transform dataset before training?\n",
    "\n",
    "class RawTransformer:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        #Mel stuff etc\n",
    "        self.config = config\n",
    "    \n",
    "    #Input is a training batch?\n",
    "    def rtoi_standard(input):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE 2021 ish\n",
    "#Instance given to DataLoader on argument batch_sampler\n",
    "\n",
    "class RandomEpisodicSampler(data.Sampler):\n",
    "    \n",
    "    #Include the option to choose the number of query samples\n",
    "    #Y_train -> labels, just a list of the targets (list of ints?)\n",
    "    def __init__(self, labels, n_episodes, n_way, n_support, n_query):\n",
    "        \n",
    "        #Number of episodes per epoch. len(labels)/(n_support * n_query) ?\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_way = n_way\n",
    "        self.n_support = n_support\n",
    "        self.n_query = n_query\n",
    "        self.n_samples = n_support+n_query\n",
    "        \n",
    "        labels = np.array(labels)\n",
    "        self.sample_indices = []\n",
    "        for i in range(max(labels) + 1):\n",
    "            ix = np.argwhere(labels == i).reshape(-1)\n",
    "            ix = torch.from_numpy(ix)\n",
    "            self.sample_indices.append(ix)\n",
    "            \n",
    "        if self.n_way > len(self.sample_indices):\n",
    "            #print(self.n_way)\n",
    "            raise ValueError('Error: \"n_way\" parameter is higher than the unique number of classes')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in range(self.n_episodes):\n",
    "            batch = []\n",
    "            classes = torch.randperm(len(self.sample_indices))[:self.n_way]\n",
    "            for c in classes:\n",
    "                #l is a list of indexes of elements in target belonging to class c\n",
    "                l = self.sample_indices[c]\n",
    "                pos = torch.randperm(len(l))[:self.n_samples]\n",
    "                batch.append(l[pos])\n",
    "            batch = torch.stack(batch).t().reshape(-1)\n",
    "            yield batch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Must somehow have access to all the data (just pass it).\n",
    "\n",
    "class ActiveEpisodicSampler(data.Sampler):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util/Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def time_2_frame(df,fps):\n",
    "\n",
    "\n",
    "    #Margin of 25 ms around the onset and offsets\n",
    "    #TODO: Should be in config\n",
    "\n",
    "    df.loc[:,'Starttime'] = df['Starttime'] - 0.025\n",
    "    df.loc[:,'Endtime'] = df['Endtime'] + 0.025\n",
    "\n",
    "    #Converting time to frames\n",
    "\n",
    "    start_time = [int(np.floor(start * fps)) for start in df['Starttime']]\n",
    "\n",
    "    end_time = [int(np.floor(end * fps)) for end in df['Endtime']]\n",
    "\n",
    "    return start_time,end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def class_to_int(labels):\n",
    "    \n",
    "    class_set = set(labels)\n",
    "    ltoix = {label:index for index, label in enumerate(class_set)}\n",
    "    return np.array([ltoix[label] for label in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "#Check over this\n",
    "def balance_class_distribution(X,Y):\n",
    "\n",
    "    '''  Class balancing through Random oversampling\n",
    "    Args:\n",
    "    -X: Feature\n",
    "    -Y: labels\n",
    "\n",
    "    Out:\n",
    "    -X_new: Feature after oversampling\n",
    "    -Y_new: Oversampled label list\n",
    "    '''\n",
    "\n",
    "    x_index = [[index] for index in range(len(X))]\n",
    "    set_y = set(Y)\n",
    "\n",
    "\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    x_unifm, y_unifm = ros.fit_resample(x_index, Y)\n",
    "    unifm_index = [index_new[0] for index_new in x_unifm]\n",
    "\n",
    "    X_new = np.array([X[index] for index in unifm_index])\n",
    "\n",
    "    sampled_index = [idx[0] for idx in x_unifm]\n",
    "    Y_new = np.array([Y[idx] for idx in sampled_index])\n",
    "\n",
    "    return X_new,Y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def norm_params(X):\n",
    "\n",
    "    '''  Normalize features\n",
    "        Args:\n",
    "        - X : Features\n",
    "\n",
    "        Out:\n",
    "        - mean : Mean of the feature set\n",
    "        - std: Standard deviation of the feature set\n",
    "        '''\n",
    "\n",
    "\n",
    "    mean = np.mean(X)\n",
    "\n",
    "    std = np.std(X)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def euclidean_dist(x, y):\n",
    "    '''\n",
    "    Compute euclidean distance between two tensors\n",
    "    '''\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    if d != y.size(1):\n",
    "        #we are currently getting stuck here?\n",
    "        #why?\n",
    "        raise Exception\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def balance_class_distribution(X,Y):\n",
    "\n",
    "    '''  Class balancing through Random oversampling\n",
    "    Args:\n",
    "    -X: Feature\n",
    "    -Y: labels\n",
    "\n",
    "    Out:\n",
    "    -X_new: Feature after oversampling\n",
    "    -Y_new: Oversampled label list\n",
    "    '''\n",
    "\n",
    "    x_index = [[index] for index in range(len(X))]\n",
    "    set_y = set(Y)\n",
    "\n",
    "\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    x_unifm, y_unifm = ros.fit_resample(x_index, Y)\n",
    "    unifm_index = [index_new[0] for index_new in x_unifm]\n",
    "\n",
    "    X_new = np.array([X[index] for index in unifm_index])\n",
    "\n",
    "    sampled_index = [idx[0] for idx in x_unifm]\n",
    "    Y_new = np.array([Y[idx] for idx in sampled_index])\n",
    "\n",
    "    return X_new,Y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "#TODO Make it possible to use all samples as negatives.\n",
    "#TODO Read up on this function, understand it better.\n",
    "\n",
    "\n",
    "def evaluate_prototypes(conf=None,hdf_eval=None,device= None,strt_index_query=None):\n",
    "\n",
    "    \"\"\" Run the evaluation\n",
    "    Args:\n",
    "     - conf: config object\n",
    "     - hdf_eval: Features from the audio file\n",
    "     - device:  cuda/cpu\n",
    "     - str_index_query : start frame of the query set w.r.t to the original file\n",
    "\n",
    "     Out:\n",
    "     - onset: Onset array predicted by the model\n",
    "     - offset: Offset array predicted by the model\n",
    "      \"\"\"\n",
    "    hop_seg = int(conf.features.hop_seg * conf.features.sr // conf.features.hop_mel)\n",
    "\n",
    "    gen_eval = TestDatagen(hdf_eval,conf)\n",
    "    X_pos, X_neg,X_query = gen_eval.generate_eval()\n",
    "\n",
    "    X_pos = torch.tensor(X_pos)\n",
    "    Y_pos = torch.LongTensor(np.zeros(X_pos.shape[0]))\n",
    "    X_neg = torch.tensor(X_neg)\n",
    "    Y_neg = torch.LongTensor(np.zeros(X_neg.shape[0]))\n",
    "    X_query = torch.tensor(X_query)\n",
    "    Y_query = torch.LongTensor(np.zeros(X_query.shape[0]))\n",
    "\n",
    "    num_batch_query = len(Y_query) // conf.eval.query_batch_size\n",
    "\n",
    "    query_dataset = torch.utils.data.TensorDataset(X_query, Y_query)\n",
    "    q_loader = torch.utils.data.DataLoader(dataset=query_dataset, batch_sampler=None,batch_size=conf.eval.query_batch_size,shuffle=False)\n",
    "    query_set_feat = torch.zeros(0,1024).cpu()\n",
    "\n",
    "\n",
    "    Model = Protonet()\n",
    "\n",
    "    if device == 'cpu':\n",
    "        Model.load_state_dict(torch.load(conf.path.best_model, map_location=torch.device('cpu')))\n",
    "    else:\n",
    "        Model.load_state_dict(torch.load(conf.path.best_model))\n",
    "\n",
    "    Model.to(device)\n",
    "    Model.eval()\n",
    "\n",
    "    'List for storing the combined probability across all iterations'\n",
    "    prob_comb = []\n",
    "\n",
    "    iterations = conf.eval.iterations\n",
    "    for i in range(iterations):\n",
    "        prob_pos_iter = []\n",
    "        neg_indices = torch.randperm(len(X_neg))[:conf.eval.samples_neg]\n",
    "        X_neg = X_neg[neg_indices]\n",
    "        Y_neg = Y_neg[neg_indices]\n",
    "        batch_size_neg = conf.eval.negative_set_batch_size\n",
    "        neg_dataset = torch.utils.data.TensorDataset(X_neg, Y_neg)\n",
    "        negative_loader = torch.utils.data.DataLoader(dataset=neg_dataset, batch_sampler=None, batch_size=batch_size_neg)\n",
    "\n",
    "        batch_samplr_pos = RandomEpisodicSampler(Y_pos, num_batch_query + 1, 1, conf.train.n_shot, conf.train.n_query)\n",
    "        pos_dataset = torch.utils.data.TensorDataset(X_pos, Y_pos)\n",
    "        pos_loader = torch.utils.data.DataLoader(dataset=pos_dataset, batch_sampler=batch_samplr_pos)\n",
    "\n",
    "        neg_iterator = iter(negative_loader)\n",
    "        pos_iterator = iter(pos_loader)\n",
    "        q_iterator = iter(q_loader)\n",
    "\n",
    "        print(\"Iteration number {}\".format(i))\n",
    "\n",
    "        for batch in tqdm(neg_iterator):\n",
    "            x_neg, y_neg = batch\n",
    "            x_neg = x_neg.to(device)\n",
    "            feat_neg = Model(x_neg)\n",
    "            feat_neg = feat_neg.detach().cpu()\n",
    "            query_set_feat = torch.cat((query_set_feat, feat_neg), dim=0)\n",
    "        neg_proto = query_set_feat.mean(dim=0)\n",
    "        neg_proto =neg_proto.to(device)\n",
    "\n",
    "        for batch in tqdm(q_iterator):\n",
    "            x_q, y_q = batch\n",
    "            x_q = x_q.to(device)\n",
    "            #Why even bother with a data loader for the positive class?\n",
    "            #Are we not only drawing the 5 sample that there is repeatedly?\n",
    "            #Could just run the positives through the network once and save the\n",
    "            #Prototype. Check that I am right about this.\n",
    "            x_pos, y_pos = next(pos_iterator)\n",
    "            x_pos = x_pos.to(device)\n",
    "            x_pos = Model(x_pos)\n",
    "            x_query = Model(x_q)\n",
    "            probability_pos = get_probability(x_pos, neg_proto, x_query)\n",
    "            prob_pos_iter.extend(probability_pos)\n",
    "\n",
    "        prob_comb.append(prob_pos_iter)\n",
    "\n",
    "    prob_final = np.mean(np.array(prob_comb),axis=0)\n",
    "\n",
    "    krn = np.array([1, -1])\n",
    "    prob_thresh = np.where(prob_final > conf.eval.p_thresh, 1, 0)\n",
    "\n",
    "    prob_pos_final = prob_final * prob_thresh\n",
    "    changes = np.convolve(krn, prob_thresh)\n",
    "\n",
    "    onset_frames = np.where(changes == 1)[0]\n",
    "    offset_frames = np.where(changes == -1)[0]\n",
    "\n",
    "    str_time_query = strt_index_query * conf.features.hop_mel / conf.features.sr\n",
    "\n",
    "    onset = (onset_frames + 1) * (hop_seg) * conf.features.hop_mel / conf.features.sr\n",
    "    onset = onset + str_time_query\n",
    "\n",
    "    offset = (offset_frames + 1) * (hop_seg) * conf.features.hop_mel / conf.features.sr\n",
    "    offset = offset + str_time_query\n",
    "\n",
    "    assert len(onset) == len(offset)\n",
    "    return onset, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def get_probability(x_pos,neg_proto,query_set_out):\n",
    "\n",
    "\n",
    "    \"\"\"Calculates the  probability of each query point belonging to either the positive or negative class\n",
    "     Args:\n",
    "     - x_pos : Model output for the positive class\n",
    "     - neg_proto : Negative class prototype calculated from randomly chosed 100 segments across the audio file\n",
    "     - query_set_out:  Model output for the first 8 samples of the query set\n",
    "\n",
    "     Out:\n",
    "     - Probabiility array for the positive class\n",
    "     \"\"\"\n",
    "\n",
    "    pos_prototype = x_pos.mean(0)\n",
    "    prototypes = torch.stack([pos_prototype,neg_proto])\n",
    "    dists = euclidean_dist(query_set_out,prototypes)\n",
    "    '''  Taking inverse distance for converting distance to probabilities'''\n",
    "    inverse_dist = torch.div(1.0, dists)\n",
    "    prob = torch.softmax(inverse_dist,dim=1)\n",
    "    '''  Probability array for positive class'''\n",
    "    prob_pos = prob[:,0]\n",
    "\n",
    "    return prob_pos.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE pretty much\n",
    "\n",
    "#Think about how to actually choose examples of the positive class in practice?\n",
    "#Can one even do this? Read paper again\n",
    "\n",
    "def dummy_choice(csv, n_shots):\n",
    "    events = []\n",
    "    for i in range(len(csv)):\n",
    "                if(csv.loc[i].values[-1] == 'POS' and len(events) < n_shots):\n",
    "                    events.append(csv.loc[i].values)\n",
    "    return events\n",
    "\n",
    "#Might wanna check the paths here and if we are please with the output.\n",
    "def post_processing(val_path, evaluation_file, new_evaluation_file, n_shots=5):\n",
    "    '''Post processing of a prediction file by removing all events that have shorter duration\n",
    "    than 60% of the minimum duration of the shots for that audio file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    val_path: path to validation set folder containing subfolders with wav audio files and csv annotations\n",
    "    evaluation_file: .csv file of predictions to be processed\n",
    "    new_evaluation_file: .csv file to be saved with predictions after post processing\n",
    "    n_shots: number of available shots\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    I think it is of great interest to not just choose the first five positives in practice.\n",
    "    Sure this is part of the challenge. But... Interesting to invesigate. Discussion about growing\n",
    "    number of supports can fit here to?\n",
    "    '''\n",
    "    \n",
    "    csv_files = [file for file in glob(os.path.join(val_path, '*.csv'))]\n",
    "    \n",
    "    dict_duration = {}\n",
    "    for csv_file in csv_files:\n",
    "        audiofile = csv_file.replace('.csv', '.wav')\n",
    "        df = pd.read_csv(csv_file)\n",
    "        events = dummy_choice(df, n_shots)\n",
    "        min_duration = 10000 #configurable?\n",
    "        for event in events:\n",
    "            if float(event[2])-float(event[1]) < min_duration:\n",
    "                min_duration = float(event[2])-float(event[1])\n",
    "        #dict_duration[audiofile] = min_duration\n",
    "        dict_duration[os.path.split(audiofile)[1]] = min_duration\n",
    "\n",
    "    results = []\n",
    "    with open(evaluation_file, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader, None)  # skip the headers\n",
    "        for row in reader:\n",
    "            results.append(row)\n",
    "\n",
    "    new_results = [['Audiofilename', 'Starttime', 'Endtime']]\n",
    "    for event in results:\n",
    "        audiofile = os.path.split(event[0])[1]\n",
    "        min_dur = dict_duration[audiofile]\n",
    "        if float(event[2])-float(event[1]) >= 0.6*min_dur:\n",
    "            new_results.append([os.path.split(event[0])[1], event[1], event[2]])\n",
    "\n",
    "    with open(new_evaluation_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(new_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at scoring files from dcase and try to understand them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, config, num_batches_tr, num_batches_val):\n",
    "    \n",
    "    if config.set.device == 'cuda':\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    #Should this be done here or passed into this function?\n",
    "    #Could be configs for more terminal flexibility\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=config.train.lr_rate)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optim, gamma=config.train.scheduler_gamma,\n",
    "                                                  step_size=config.train.scheduler_step_size)\n",
    "    num_epochs = config.train.epochs\n",
    "    \n",
    "    best_model_path = config.path.best_model\n",
    "    last_model_path = config.path.last_model\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    best_val_acc = 0.0\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print('Epoch {}'.format(epoch))\n",
    "        train_iterator = iter(train_loader)\n",
    "        for batch in tqdm(train_iterator):\n",
    "            optim.zero_grad()\n",
    "            model.train()\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            x_out = model(x)\n",
    "            tr_loss, tr_acc = prototypical_loss(x_out, y, config.train.n_shot)\n",
    "            train_loss.append(tr_loss.item())\n",
    "            train_acc.append(tr_acc.item())\n",
    "            \n",
    "            tr_loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "        avg_loss_tr = np.mean(train_loss[-num_batches_tr:])\n",
    "        avg_acc_tr = np.mean(train_acc[-num_batches_tr:])\n",
    "        print('Average train loss: {}  Average training accuracy: {}'.format(avg_loss_tr,avg_acc_tr))\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        #No dropouts in model for now, I think there is no difference between train and eval mode\n",
    "        model.eval()\n",
    "        val_iterator = iter(val_loader)\n",
    "        for batch in tqdm(val_iterator):\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            x_val = model(x)\n",
    "            valid_loss, valid_acc = prototypical_loss(x_val, y, config.train.n_shot)\n",
    "            val_loss.append(valid_loss.item())\n",
    "            val_acc.append(valid_acc.item())\n",
    "        avg_loss_val = np.mean(val_loss[-num_batches_val:])\n",
    "        avg_acc_val = np.mean(val_acc[-num_batches_val:])\n",
    "        \n",
    "        print ('Epoch {}, Validation loss {:.4f}, Validation accuracy {:.4f}'.format(epoch,avg_loss_val,avg_acc_val))\n",
    "        if avg_acc_val > best_val_acc:\n",
    "            print(\"Saving the best model with valdation accuracy {}\".format(avg_acc_val))\n",
    "            best_val_acc = avg_acc_val\n",
    "            best_state = model.state_dict()\n",
    "            torch.save(model.state_dict(),best_model_path)\n",
    "    torch.save(model.state_dict(),last_model_path)\n",
    "\n",
    "    return best_val_acc, model, best_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def eval(config):\n",
    "    \n",
    "    if config.set.device == 'cuda':\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "    name_arr = np.array([])\n",
    "    onset_arr = np.array([])\n",
    "    offset_arr = np.array([])\n",
    "    all_feat_files = [file for file in glob(os.path.join(config.path.test_w,'*.h5'))]\n",
    "\n",
    "    for feat_file in all_feat_files:\n",
    "        feat_name = feat_file.split('/')[-1]\n",
    "        audio_name = feat_name.replace('h5','wav')\n",
    "\n",
    "        print(\"Processing audio file : {}\".format(audio_name))\n",
    "\n",
    "        hdf_eval = h5py.File(feat_file,'r')\n",
    "        strt_index_query =  hdf_eval['start_index_query'][:][0]\n",
    "        onset,offset = evaluate_prototypes(config, hdf_eval, device, strt_index_query)\n",
    "\n",
    "        name = np.repeat(audio_name,len(onset))\n",
    "        name_arr = np.append(name_arr,name)\n",
    "        onset_arr = np.append(onset_arr,onset)\n",
    "        offset_arr = np.append(offset_arr,offset)\n",
    "\n",
    "    df_out = pd.DataFrame({'Audiofilename':name_arr,'Starttime':onset_arr,'Endtime':offset_arr})\n",
    "    csv_path = os.path.join(config.path.root,'Eval_out.csv')\n",
    "    df_out.to_csv(csv_path,index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-534a65161f8f>:1: UserWarning: config_path is not specified in hydra.initialize().\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_hydra_main_config_path for more information.\n",
      "  initialize(job_name='test')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize(job_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name='config')\n",
    "s = Spectralizer(cfg)\n",
    "f_ext = MyF_Ext(cfg, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_ext.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_ext.extract_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f2ebaa13aab7>:19: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  hf = h5py.File(os.path.join(config.path.train_w, 'mel_train.h5'))\n"
     ]
    }
   ],
   "source": [
    "data_gen = Datagen(cfg)\n",
    "X_train, Y_train, X_val, Y_val = data_gen.generate_train()\n",
    "X_tr = torch.tensor(X_train)\n",
    "Y_tr = torch.LongTensor(Y_train)\n",
    "X_val = torch.tensor(X_val)\n",
    "Y_val = torch.LongTensor(Y_val)\n",
    "samples_per_cls = cfg.train.n_shot + cfg.train.n_query\n",
    "batch_size_tr = samples_per_cls * cfg.train.k_way\n",
    "batch_size_vd = batch_size_tr\n",
    "\n",
    "num_batches_tr = len(Y_train)//batch_size_tr\n",
    "num_batches_vd = len(Y_val)//batch_size_vd\n",
    "\n",
    "\n",
    "samplr_train = RandomEpisodicSampler(Y_train,num_batches_tr,cfg.train.k_way, cfg.train.n_shot, cfg.train.n_query)\n",
    "samplr_valid = RandomEpisodicSampler(Y_val,num_batches_vd,cfg.train.k_way,cfg.train.n_shot, cfg.train.n_query)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_tr,Y_tr)\n",
    "valid_dataset = torch.utils.data.TensorDataset(X_val,Y_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_sampler=samplr_train,num_workers=0,pin_memory=True,shuffle=False)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_sampler=samplr_valid,num_workers=0,pin_memory=True,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                             | 0/828 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-68abaef57ec2>:14: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  supp_idxs = list(map(lambda c: target_cpu.eq(c).nonzero()[:n_support].squeeze(1), classes))\n",
      "100%|| 828/828 [00:54<00:00, 15.13it/s]\n",
      "  2%|                                                    | 5/276 [00:00<00:06, 43.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 1.8727535148430166  Average training accuracy: 0.8214009622660812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [00:04<00:00, 61.91it/s]\n",
      "  0%|                                                     | 1/828 [00:00<01:23,  9.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Validation loss 0.3836, Validation accuracy 0.8879\n",
      "Saving the best model with valdation accuracy 0.8878985472779343\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 828/828 [00:52<00:00, 15.70it/s]\n",
      "  2%|                                                    | 5/276 [00:00<00:06, 43.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.2705789223338077  Average training accuracy: 0.9123188368076287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [00:04<00:00, 61.51it/s]\n",
      "  0%|                                                    | 3/828 [00:00<00:43, 19.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation loss 0.2213, Validation accuracy 0.9221\n",
      "Saving the best model with valdation accuracy 0.9221014473317326\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 828/828 [00:52<00:00, 15.73it/s]\n",
      "  2%|                                                    | 5/276 [00:00<00:06, 44.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.17672864548301837  Average training accuracy: 0.937342993159225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [00:04<00:00, 62.46it/s]\n",
      "  0%|                                                    | 3/828 [00:00<00:42, 19.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Validation loss 0.1831, Validation accuracy 0.9325\n",
      "Saving the best model with valdation accuracy 0.9325362301391104\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 828/828 [00:53<00:00, 15.61it/s]\n",
      "  2%|                                                    | 5/276 [00:00<00:06, 43.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.14961263474615757  Average training accuracy: 0.9448550724320941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [00:04<00:00, 60.53it/s]\n",
      "  0%|                                                    | 3/828 [00:00<00:45, 18.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Validation loss 0.1339, Validation accuracy 0.9491\n",
      "Saving the best model with valdation accuracy 0.9491304364325344\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 828/828 [00:53<00:00, 15.45it/s]\n",
      "  2%|                                                    | 5/276 [00:00<00:06, 44.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.11909365056477406  Average training accuracy: 0.9560386475157623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [00:04<00:00, 61.29it/s]\n",
      "  0%|                                                    | 3/828 [00:00<00:43, 19.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Validation loss 0.1168, Validation accuracy 0.9522\n",
      "Saving the best model with valdation accuracy 0.9522463765697203\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 828/828 [00:52<00:00, 15.69it/s]\n",
      "  1%|                                                    | 4/276 [00:00<00:06, 39.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.10226054022728087  Average training accuracy: 0.9614251215676755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [00:04<00:00, 58.93it/s]\n",
      "  0%|                                                    | 3/828 [00:00<00:45, 18.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Validation loss 0.1064, Validation accuracy 0.9580\n",
      "Saving the best model with valdation accuracy 0.9579710154861644\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 828/828 [00:52<00:00, 15.67it/s]\n",
      "  2%|                                                    | 5/276 [00:00<00:06, 44.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.10062901750063537  Average training accuracy: 0.9610144943718749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [00:04<00:00, 61.53it/s]\n",
      "  0%|                                                    | 3/828 [00:00<00:40, 20.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Validation loss 0.1080, Validation accuracy 0.9587\n",
      "Saving the best model with valdation accuracy 0.958695654635844\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 828/828 [00:50<00:00, 16.49it/s]\n",
      "  2%|                                                    | 5/276 [00:00<00:05, 46.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.08349241159881908  Average training accuracy: 0.9683816435832332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [00:04<00:00, 60.91it/s]\n",
      "  0%|                                                    | 3/828 [00:00<00:40, 20.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Validation loss 0.0846, Validation accuracy 0.9689\n",
      "Saving the best model with valdation accuracy 0.9689130450504414\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 828/828 [00:50<00:00, 16.54it/s]\n",
      "  2%|                                                    | 5/276 [00:00<00:05, 45.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.0741574721717937  Average training accuracy: 0.9724637693253116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [00:04<00:00, 63.01it/s]\n",
      "  0%|                                                    | 3/828 [00:00<00:40, 20.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Validation loss 0.1073, Validation accuracy 0.9611\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 828/828 [00:50<00:00, 16.46it/s]\n",
      "  2%|                                                    | 5/276 [00:00<00:06, 43.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.06717028103495784  Average training accuracy: 0.9752898567540634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [00:04<00:00, 60.75it/s]\n",
      "  0%|                                                    | 3/828 [00:00<00:42, 19.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Validation loss 0.0821, Validation accuracy 0.9701\n",
      "Saving the best model with valdation accuracy 0.9701449292293494\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 828/828 [00:52<00:00, 15.86it/s]\n",
      "  1%|                                                    | 4/276 [00:00<00:06, 39.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.0469274992562888  Average training accuracy: 0.9832608714195841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [00:04<00:00, 63.50it/s]\n",
      "  0%|                                                    | 3/828 [00:00<00:42, 19.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Validation loss 0.0580, Validation accuracy 0.9775\n",
      "Saving the best model with valdation accuracy 0.9774637688761172\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 828/828 [00:50<00:00, 16.38it/s]\n",
      "  2%|                                                    | 5/276 [00:00<00:05, 47.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.038373080432454694  Average training accuracy: 0.9851932388860822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [00:04<00:00, 63.87it/s]\n",
      "  0%|                                                    | 3/828 [00:00<00:41, 19.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Validation loss 0.0491, Validation accuracy 0.9833\n",
      "Saving the best model with valdation accuracy 0.9832608719234881\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 828/828 [00:50<00:00, 16.52it/s]\n",
      "  2%|                                                    | 5/276 [00:00<00:05, 45.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.0432527793858103  Average training accuracy: 0.9848550745254554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [00:04<00:00, 62.92it/s]\n",
      "  0%|                                                    | 3/828 [00:00<00:40, 20.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Validation loss 0.0500, Validation accuracy 0.9828\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 828/828 [00:50<00:00, 16.27it/s]\n",
      "  2%|                                                    | 5/276 [00:00<00:06, 43.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.036954542066862064  Average training accuracy: 0.9863768129964957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [00:04<00:00, 59.99it/s]\n",
      "  0%|                                                    | 3/828 [00:00<00:44, 18.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Validation loss 0.0444, Validation accuracy 0.9838\n",
      "Saving the best model with valdation accuracy 0.9838405819474787\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 828/828 [00:53<00:00, 15.52it/s]\n",
      "  2%|                                                    | 5/276 [00:00<00:05, 46.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.033319689293192636  Average training accuracy: 0.9882367174838476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 276/276 [00:04<00:00, 61.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Validation loss 0.0458, Validation accuracy 0.9831\n",
      "Best accuracy of the model on training set is 0.9838405819474787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Protonet()\n",
    "best_acc,model,best_state = train(model,train_loader,valid_loader,cfg,num_batches_tr,num_batches_vd)\n",
    "print(\"Best accuracy of the model on training set is {}\".format(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio file : C:\\Users\\martin\\Documents\\playground\\data\\hfiles\\test\\a1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f2ebaa13aab7>:19: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  hf = h5py.File(os.path.join(config.path.train_w, 'mel_train.h5'))\n",
      "100%|| 41/41 [00:00<00:00, 304.52it/s]\n",
      "  0%|                                                            | 0/8822 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8822/8822 [00:29<00:00, 302.33it/s]\n",
      "100%|| 41/41 [00:00<00:00, 281.57it/s]\n",
      "  0%|                                                            | 0/8822 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8822/8822 [00:30<00:00, 292.09it/s]\n",
      "100%|| 41/41 [00:00<00:00, 237.63it/s]\n",
      "  0%|                                                            | 0/8822 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8822/8822 [00:29<00:00, 302.93it/s]\n",
      "<ipython-input-8-f2ebaa13aab7>:19: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  hf = h5py.File(os.path.join(config.path.train_w, 'mel_train.h5'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio file : C:\\Users\\martin\\Documents\\playground\\data\\hfiles\\test\\BUK1_20181011_001004.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 41/41 [00:00<00:00, 334.22it/s]\n",
      "  0%|                                                            | 0/3863 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3863/3863 [00:13<00:00, 296.02it/s]\n",
      "100%|| 41/41 [00:00<00:00, 268.69it/s]\n",
      "  0%|                                                            | 0/3863 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3863/3863 [00:12<00:00, 319.89it/s]\n",
      "100%|| 41/41 [00:00<00:00, 255.34it/s]\n",
      "  0%|                                                            | 0/3863 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3863/3863 [00:11<00:00, 322.47it/s]\n",
      "<ipython-input-8-f2ebaa13aab7>:19: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  hf = h5py.File(os.path.join(config.path.train_w, 'mel_train.h5'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio file : C:\\Users\\martin\\Documents\\playground\\data\\hfiles\\test\\BUK1_20181013_023504.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 41/41 [00:00<00:00, 342.58it/s]\n",
      "  0%|                                                            | 0/4215 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4215/4215 [00:11<00:00, 372.30it/s]\n",
      "100%|| 41/41 [00:00<00:00, 270.46it/s]\n",
      "  0%|                                                            | 0/4215 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4215/4215 [00:11<00:00, 367.01it/s]\n",
      "100%|| 41/41 [00:00<00:00, 246.17it/s]\n",
      "  0%|                                                            | 0/4215 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4215/4215 [00:11<00:00, 354.61it/s]\n",
      "<ipython-input-8-f2ebaa13aab7>:19: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  hf = h5py.File(os.path.join(config.path.train_w, 'mel_train.h5'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio file : C:\\Users\\martin\\Documents\\playground\\data\\hfiles\\test\\BUK4_20171022_004304a.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 41/41 [00:00<00:00, 311.43it/s]\n",
      "  0%|                                                            | 0/3007 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3007/3007 [00:09<00:00, 309.41it/s]\n",
      "100%|| 41/41 [00:00<00:00, 258.55it/s]\n",
      "  0%|                                                            | 0/3007 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3007/3007 [00:09<00:00, 308.95it/s]\n",
      " 56%|                      | 23/41 [00:00<00:00, 228.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 41/41 [00:00<00:00, 204.53it/s]\n",
      "100%|| 3007/3007 [00:09<00:00, 309.48it/s]\n",
      "<ipython-input-8-f2ebaa13aab7>:19: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  hf = h5py.File(os.path.join(config.path.train_w, 'mel_train.h5'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio file : C:\\Users\\martin\\Documents\\playground\\data\\hfiles\\test\\BUK5_20161101_002104a.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 41/41 [00:00<00:00, 334.23it/s]\n",
      "  0%|                                                            | 0/3914 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3914/3914 [00:12<00:00, 307.71it/s]\n",
      "100%|| 41/41 [00:00<00:00, 265.22it/s]\n",
      "  0%|                                                            | 0/3914 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3914/3914 [00:12<00:00, 307.89it/s]\n",
      " 59%|                     | 24/41 [00:00<00:00, 238.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 41/41 [00:00<00:00, 229.66it/s]\n",
      "100%|| 3914/3914 [00:12<00:00, 309.22it/s]\n",
      "<ipython-input-8-f2ebaa13aab7>:19: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  hf = h5py.File(os.path.join(config.path.train_w, 'mel_train.h5'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio file : C:\\Users\\martin\\Documents\\playground\\data\\hfiles\\test\\BUK5_20180921_015906a.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 41/41 [00:00<00:00, 321.17it/s]\n",
      "  0%|                                                            | 0/4194 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4194/4194 [00:12<00:00, 334.51it/s]\n",
      "100%|| 41/41 [00:00<00:00, 275.91it/s]\n",
      "  0%|                                                            | 0/4194 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4194/4194 [00:12<00:00, 338.15it/s]\n",
      " 59%|                     | 24/41 [00:00<00:00, 229.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 41/41 [00:00<00:00, 225.88it/s]\n",
      "100%|| 4194/4194 [00:12<00:00, 336.76it/s]\n",
      "<ipython-input-8-f2ebaa13aab7>:19: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  hf = h5py.File(os.path.join(config.path.train_w, 'mel_train.h5'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio file : C:\\Users\\martin\\Documents\\playground\\data\\hfiles\\test\\n1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 41/41 [00:00<00:00, 339.75it/s]\n",
      "  0%|                                                            | 0/3378 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3378/3378 [00:11<00:00, 304.99it/s]\n",
      "100%|| 41/41 [00:00<00:00, 277.77it/s]\n",
      "  0%|                                                            | 0/3378 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3378/3378 [00:10<00:00, 307.17it/s]\n",
      "100%|| 41/41 [00:00<00:00, 241.82it/s]\n",
      "  0%|                                                            | 0/3378 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3378/3378 [00:11<00:00, 305.28it/s]\n"
     ]
    }
   ],
   "source": [
    "eval(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "post_processing(cfg.path.data_test, os.path.join(cfg.path.root, 'Eval_out.csv'), 'pp.csv',\n",
    "               cfg.train.n_shot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
