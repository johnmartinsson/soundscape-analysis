{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2614c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import mir_eval\n",
    "import torch\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tqdm import tqdm\n",
    "import datasets.feature_extract as fe\n",
    "import datasets.randomepisode as re\n",
    "import datasets.activeepisode as ae\n",
    "import datasets.data_gen as dg\n",
    "import datasets.smoothquery as sq\n",
    "import datasets.specaugmentset as sas\n",
    "import models.prototypical as pt\n",
    "from sklearn.cluster import  KMeans\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import utils\n",
    "import math\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import copy\n",
    "from scipy import stats\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "import scipy\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "022b50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd97834",
   "metadata": {},
   "source": [
    "### Post processing entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13219bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 31/31 [03:17<00:00,  6.38s/it]\n"
     ]
    }
   ],
   "source": [
    "def post_processing():\n",
    "    \n",
    "    predictions_file = open('postprocessing/G_TEST_out.csv', newline='')\n",
    "    new_predictions_file = 'postprocessing/G_PP_TEST_out.csv'\n",
    "    \n",
    "    predictions_reader = csv.reader(predictions_file, delimiter=',')\n",
    "    predictions = []\n",
    "    for e in predictions_reader:\n",
    "        predictions.append(e)\n",
    "    \n",
    "    #Median filtering\n",
    "    predictions = median_filtering(predictions)\n",
    "    #Remove short duration events \n",
    "    predictions = remove_short(predictions)\n",
    "    \n",
    "    \n",
    "    with open(new_predictions_file, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(predictions)\n",
    "\n",
    "post_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0cd489",
   "metadata": {},
   "source": [
    "### Median filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "372c29b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_filtering(predictions):\n",
    "    \n",
    "    #Number of ticks per second\n",
    "    rate = 12000\n",
    "    increment = 1.0/rate\n",
    "    \n",
    "    num_proc=16\n",
    "    pool = Pool(processes=num_proc)\n",
    "    \n",
    "    #dict with prediction onset and offsets split over audiofiles\n",
    "    pred_dict = defaultdict(list)\n",
    "    \n",
    "    #Skip header\n",
    "    for i in range(1,len(predictions)):\n",
    "        pred_dict[predictions[i][0]].append([predictions[i][1], predictions[i][2]])\n",
    "    \n",
    "    new_predictions = []\n",
    "    #Attach header\n",
    "    new_predictions.append(predictions[0])\n",
    "    \n",
    "    print('Median filtering')\n",
    "    \n",
    "    for key in tqdm(pred_dict.keys()):\n",
    "        \n",
    "        #Go from onset offset predictions to array with binary elements indicating events\n",
    "        index = 0.0\n",
    "        prediction_array = []\n",
    "        for event in pred_dict[key]:\n",
    "            while index < float(event[0]):\n",
    "                prediction_array += [0]\n",
    "                index += increment\n",
    "            while index < float(event[1]):\n",
    "                prediction_array += [1]\n",
    "                index += increment\n",
    "        \n",
    "        #How should we think around this window size?\n",
    "        #Can we base it per file?\n",
    "        data_test = '/home/willbo/data/dcase/test'\n",
    "        n_shots = 5\n",
    "        csv_files = [file for file in glob(os.path.join(data_test, '*.csv'))]\n",
    "        dict_duration = {}\n",
    "        for csv_file in csv_files:\n",
    "            audiofile = csv_file.replace('.csv', '.wav')\n",
    "            df = pd.read_csv(csv_file)\n",
    "            events = dummy_choice(df, n_shots)\n",
    "            duration = 0.0 #configurable?\n",
    "            for event in events:\n",
    "                duration += float(event[2])-float(event[1])\n",
    "                    \n",
    "            #dict_duration[audiofile] = min_duration\n",
    "            #One third of the average duration of the shots. Use this as window lenght, ezi pizi.\n",
    "            #In seconds\n",
    "            dict_duration[os.path.split(audiofile)[1]] = 0.33*(duration/n_shots)\n",
    "        #print('Using window size: '+str(dict_duration[key])+' for file: '+key)\n",
    "        w_size = dict_duration[key] #in s, whole size of window, so from left of index to right = 0.05s\n",
    "        \n",
    "        prediction_array = np.array(prediction_array)\n",
    "        w_size = math.floor(w_size/increment)\n",
    "        \n",
    "        if w_size % 2 == 0:\n",
    "            w_size += 1\n",
    "        \n",
    "        d = math.floor(len(prediction_array)/num_proc)\n",
    "        \n",
    "        '''\n",
    "        TODO: Obs this method brings us extremely close to mem error. This is not really a sustainable approach.\n",
    "        We need to do some mem management code here probably.\n",
    "        It looks like the notebook stalls.\n",
    "        Chaning the rate to 12000 seems to do the trick. Should still be granular enough.\n",
    "        '''\n",
    "        \n",
    "        ixs = []\n",
    "        for i in range(num_proc):\n",
    "            if i == (num_proc-1):\n",
    "                ixs += [list(range(i*d-int((w_size-1)/2), len(prediction_array)))]\n",
    "            else:    \n",
    "                ixs += [list(range(max(0, i*d-int((w_size-1)/2)), (i+1)*d+int((w_size-1)/2)))]\n",
    "\n",
    "        worker_output = pool.map(worker_filter_array, [(int(w_size), prediction_array[ix]) for ix in ixs])\n",
    "        w_filtered_array = []\n",
    "        \n",
    "        for i in range(len(worker_output)):\n",
    "            if i == 0:\n",
    "                w_filtered_array += [worker_output[i][0:len(worker_output[i])-int((w_size-1)/2)]]\n",
    "            elif i == len(worker_output)-1:\n",
    "                w_filtered_array += [worker_output[i][int((w_size-1)/2):len(worker_output[i])]]\n",
    "            else:\n",
    "                w_filtered_array += [worker_output[i][int((w_size-1)/2):len(worker_output[i])-int((w_size-1)/2)]]\n",
    "        \n",
    "        \n",
    "        #TODO: Take care of warning here. This is deprecated. Gotta flatten it outside of numpy i think!\n",
    "        filtered_array = np.zeros(0)\n",
    "        for e in w_filtered_array:\n",
    "            filtered_array = np.append(filtered_array, e)\n",
    "        #filtered_array = median_filter_array(prediction_array, math.floor(w_size/(increment)))\n",
    "        #Reverse from array with binary elements indicating events to onset offset predictions\n",
    "        krn = [1, -1]\n",
    "        changes = np.convolve(filtered_array, krn)\n",
    "        #This is deprecated and fails for some reason?\n",
    "        onsets = np.where(changes == 1)\n",
    "        offsets = np.where(changes == -1)\n",
    "        events = list(zip(onsets[0], offsets[0]))\n",
    "        new_events = []\n",
    "        for e in events:\n",
    "            new_events.append([e[0]*increment, e[1]*increment])\n",
    "        pred_dict[key] = new_events\n",
    "        \n",
    "    \n",
    "    #Assume new predictions to be in a dictionary with files as keys and elements being [onset, offset]\n",
    "    for key in pred_dict.keys():\n",
    "        for e in pred_dict[key]:\n",
    "            new_predictions.append([key, e[0], e[1]])\n",
    "    \n",
    "    return new_predictions\n",
    "\n",
    "def median_filter_array(array, window_size):\n",
    "    return scipy.ndimage.median_filter(array, size=window_size, mode='nearest')\n",
    "\n",
    "def worker_filter_array(t):\n",
    "    return scipy.ndimage.median_filter(t[1], size=t[0], mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef498d5",
   "metadata": {},
   "source": [
    "### Remove short duration events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "078f397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_choice(csv, n_shots):\n",
    "    events = []\n",
    "    for i in range(len(csv)):\n",
    "                if(csv.loc[i].values[-1] == 'POS' and len(events) < n_shots):\n",
    "                    events.append(csv.loc[i].values)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c4bf3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short(predictions):\n",
    "    data_test = '/home/willbo/data/dcase/test'\n",
    "    n_shots = 5\n",
    "    \n",
    "    csv_files = [file for file in glob(os.path.join(data_test, '*.csv'))]\n",
    "    dict_duration = {}\n",
    "    for csv_file in csv_files:\n",
    "        audiofile = csv_file.replace('.csv', '.wav')\n",
    "        df = pd.read_csv(csv_file)\n",
    "        events = dummy_choice(df, n_shots)\n",
    "        min_duration = 10000 #configurable?\n",
    "        for event in events:\n",
    "            if float(event[2])-float(event[1]) < min_duration:\n",
    "                min_duration = float(event[2])-float(event[1])\n",
    "        #dict_duration[audiofile] = min_duration\n",
    "        dict_duration[os.path.split(audiofile)[1]] = min_duration\n",
    "    \n",
    "    '''\n",
    "    results = []\n",
    "    with open(evaluation_file, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader, None)  # skip the headers\n",
    "        for row in reader:\n",
    "            results.append(row)\n",
    "    '''\n",
    "    \n",
    "    new_results = [['Audiofilename', 'Starttime', 'Endtime']]\n",
    "    for event in predictions[1:]:\n",
    "        audiofile = os.path.split(event[0])[1]\n",
    "        min_dur = dict_duration[audiofile]\n",
    "        if float(event[2])-float(event[1]) >= 0.6*min_dur:\n",
    "            new_results.append([os.path.split(event[0])[1], event[1], event[2]])\n",
    "\n",
    "    return new_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "477d81de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[1. 1. 1. 1. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "kr = [1, -1]\n",
    "a = [0,0,0,1,1,1,1,0,0,1,1,0]\n",
    "changes = np.convolve(a, kr)\n",
    "on = np.where(changes == 1)\n",
    "off = np.where(changes == -1)\n",
    "events = list(zip(on[0], off[0]))\n",
    "\n",
    "\n",
    "t = np.zeros(0)\n",
    "print(t)\n",
    "t = np.append(t, np.array([1,1,1,1]))\n",
    "t = np.append(t, np.array([2,2,2,2]))\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
