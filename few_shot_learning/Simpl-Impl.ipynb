{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import abc\n",
    "import yaml\n",
    "import h5py\n",
    "import librosa\n",
    "import os\n",
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from glob import glob\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHow to make the framework flexible enough that one can point to which samples in a batch are meant to be\\nsupport/query per class? The implementation in DCASE2021 does not handle this.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "How to make the framework flexible enough that one can point to which samples in a batch are meant to be\n",
    "support/query per class? The implementation in DCASE2021 does not handle this.\n",
    "\n",
    "\n",
    "Currently return the pcen transposed. Where to transpose it back?\n",
    "Batcher? Most important thing is just to not forget i think.\n",
    "\n",
    "The code only allows one positive class per segment for now I think.\n",
    "This might be something we would like to fix?\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototypical net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE2021\n",
    "\n",
    "def conv_block(in_channels,out_channels):\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels,out_channels,3,padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE2021\n",
    "\n",
    "#TODO introduce parametrization of conv blocks?\n",
    "class Protonet(nn.Module):\n",
    "    def __init__(self, raw_transformer=None):\n",
    "        super(Protonet,self).__init__()\n",
    "        self.raw_transformer = raw_transformer\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv_block(1,128),\n",
    "            conv_block(128,128),\n",
    "            conv_block(128,128),\n",
    "            conv_block(128,128)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        #Is there risk for this to be super slow?\n",
    "        #A naive approach might transform the same data more than once?\n",
    "        #Lookup tables?\n",
    "        if self.raw_transformer is not None:\n",
    "            x = self.raw_transformer.rtoi_standard(x)\n",
    "        (num_samples,seq_len,mel_bins) = x.shape\n",
    "        x = x.view(-1,1,seq_len,mel_bins)\n",
    "        x = self.encoder(x)\n",
    "        return x.view(x.size(0),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Will most likely lean heavily on the implementation of the DCASE2021 task 5 baseline implementation.\n",
    "\n",
    "'''\n",
    "def prototypical_loss(input, target, n_support, supp_idxs=None):\n",
    "    \n",
    "    target_cpu = target.to('cpu')\n",
    "    input_cpu = input.to('cpu')\n",
    "    classes = torch.unique(target_cpu)\n",
    "    n_classes = len(classes)\n",
    "    n_query = target.eq(classes[0].item()).sum().item() - n_support\n",
    "    if supp_idxs is None:\n",
    "        #Rewrite, need to select only n_support. We might have n_query > n_support\n",
    "        supp_idxs = torch.stack(list(map(lambda c: target_cpu.eq(c).nonzero()[:n_support], classes))).squeeze(1)\n",
    "        q_idxs = torch.stack(list(map(lambda c: target_cpu.eq(c).nonzero()[n_support:], classes))).view(-1)\n",
    "    else:\n",
    "        #Work from supp_idxs\n",
    "        q_idxs = None\n",
    "        \n",
    "    prototypes = torch.stack([input_cpu[idx_list].mean(0) for idx_list in supp_idxs])\n",
    "    query_samples = input_cpu[q_idxs]\n",
    "    dists = euclidean_dist(query_samples, prototypes)\n",
    "    \n",
    "    #Check\n",
    "    log_p_y = F.log_softmax(-dists, dim=1).view(n_classes, n_query, -1)\n",
    "    target_inds = torch.arange(0, n_classes)\n",
    "    target_inds = target_inds.view(n_classes, 1, 1)\n",
    "    target_inds = target_inds.expand(n_classes, n_query, 1).long()\n",
    "    #.mean() -> 1/NcNq\n",
    "    loss_val = -log_p_y.gather(2, target_inds).squeeze().view(-1).mean()\n",
    "    _, y_hat = log_p_y.max(2)\n",
    "    acc_val = y_hat.eq(target_inds.squeeze()).float().mean()\n",
    "    return loss_val, acc_val\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    * Design choice: Handle most of pre-processing as part of the model (torchlibrosa)?\\n      May ultimately lead to simpler augmentation etc down the line. Work with raw audio as far as possible?\\n      \\n    * Make use of h5py library for storing training, validation and test sets?\\n      Still raw audio sets?\\n    \\n    * Incorporate pytorch Dataloader, seems prudent and a good design choice.\\n      read(h5py) file + Episodic sampler -> Dataloader?\\n      \\n    * Slight change of mind. Datagen and FeatureExtractor is not really worth spending time on for now.\\n      Sure they could be interfaces for a framework up the road but can do without for now since the loop\\n      will most likely be quite task dependent for now.\\n      \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    * Design choice: Handle most of pre-processing as part of the model (torchlibrosa)?\n",
    "      May ultimately lead to simpler augmentation etc down the line. Work with raw audio as far as possible?\n",
    "      \n",
    "    * Make use of h5py library for storing training, validation and test sets?\n",
    "      Still raw audio sets?\n",
    "    \n",
    "    * Incorporate pytorch Dataloader, seems prudent and a good design choice.\n",
    "      read(h5py) file + Episodic sampler -> Dataloader?\n",
    "      \n",
    "    * Slight change of mind. Datagen and FeatureExtractor is not really worth spending time on for now.\n",
    "      Sure they could be interfaces for a framework up the road but can do without for now since the loop\n",
    "      will most likely be quite task dependent for now.\n",
    "      \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Possibly take a h5 file as input and return X_train, Y_train, X_val, Y_val\n",
    "Is this an approach that we like? Is it commonly used for deep learning?\n",
    "'''\n",
    "\n",
    "#Heavily DCASE inspired. I think what they did is ok though.\n",
    "class Datagen():\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        if config.features.raw:\n",
    "            #These obviosly requires more processing down the pipe but that is application dependent.\n",
    "            hf = h5py.File(os.path.join(config.path.train_w, 'raw_train.h5'))\n",
    "        else:\n",
    "            hf = h5py.File(os.path.join(config.path.train_w, 'mel_train.h5'))\n",
    "            self.x = hf['features'][:]\n",
    "            self.labels = [s.decode() for s in hf['labels'][:]]\n",
    "            if config.datagen.ltoi:\n",
    "                self.y = class_to_int(self.labels)\n",
    "            else:\n",
    "                self.y = self.labels\n",
    "            if config.datagen.balance:\n",
    "                self.x, self.y = balance_class_distribtuion(self.x, self.y)\n",
    "            \n",
    "            array_train = np.arange(len(self.x))\n",
    "            if config.datagen.stratify:\n",
    "                _,_,_,_,train_array,valid_array = train_test_split(self.x, self.y, array_train, \\\n",
    "                                                    random_state=config.datagen.random_state, stratify=self.y)\n",
    "            else:\n",
    "                _,_,_,_,train_array,valid_array = train_test_split(self.x, self.y, array_train, \\\n",
    "                                                    random_state=config.datagen.random_state)\n",
    "                \n",
    "            self.train_index = train_array\n",
    "            self.valid_index = valid_array\n",
    "            if config.datagen.normalize:\n",
    "                self.mean, self.std = norm_params(self.x[train_array])\n",
    "            else:\n",
    "                self.mean = None\n",
    "                self.std = None\n",
    "                \n",
    "    def feature_scale(self, x):\n",
    "        return (x - self.mean)/self.std\n",
    "    \n",
    "    def generate_train(self):\n",
    "        train_array = sorted(self.train_index)\n",
    "        valid_array = sorted(self.valid_index)\n",
    "        X_train = self.x[train_array]\n",
    "        Y_train = self.y[train_array]\n",
    "        X_val = self.x[valid_array]\n",
    "        Y_val = self.y[valid_array]\n",
    "        if self.config.datagen.normalize:\n",
    "            X_train = self.feature_scale(X_train)\n",
    "            X_val = self.feature_scale(X_val)\n",
    "        return X_train, Y_train, X_val, Y_val\n",
    "        \n",
    "\n",
    "class TestDatagen(Datagen):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This could be an interface / abstract class to build audio \n",
    "to some other format instance to plug into feature extractor\n",
    "'''\n",
    "\n",
    "class Spectralizer():\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "        self.sr = config.features.sr\n",
    "        self.n_fft = config.features.n_fft\n",
    "        self.hop = config.features.hop_mel\n",
    "        self.n_mels = config.features.n_mels\n",
    "        self.fmax = config.features.fmax\n",
    "        \n",
    "\n",
    "    def raw_to_spec(self, audio, config):\n",
    "\n",
    "        #Supposedly suggested by librosa.\n",
    "        audio = audio * (2**32)\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(audio, sr=self.sr, n_fft=self.n_fft, hop_length=self.hop,\n",
    "                                                 n_mels=self.n_mels, fmax=self.fmax)\n",
    "\n",
    "        pcen = librosa.core.pcen(mel_spec, sr=self.sr)\n",
    "        pcen = pcen.astype(np.float32)\n",
    "        \n",
    "        #Note that we transform the features here and therefor have time/frame along dim 0.\n",
    "        #Transform back when loading data? Smaksak\n",
    "        return pcen.T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Possibly work on an raw files and annotations and return/write h5 files.\n",
    "This might be clunky to include in a framework since this most likely is dataset dependent.\n",
    "Might however benfit from having an interface which is inherited by classes working on specific datasets.\n",
    "'''\n",
    "\n",
    "class FeatureExtractor(abc.ABC):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "'''\n",
    "Takes the data from the DCASE (all files one folder) and returns h5 file with the datasets 'features' and 'labels'\n",
    "This takes no heed to unlabeled segments and therefor we will have no unlabeled data to work with.\n",
    "This is an interesting TODO. Most likely need to rework some of the mechanisms here to work with limited RAM.\n",
    "Extract segment -> write to file etc... Look at DCASE code for example\n",
    "Unlabeled data could be saved to a new dataset 'unlabeled' for example.\n",
    "\n",
    "\n",
    "TODO: MemError already present even before processing unlabeled data and only one of the smaller audio files.\n",
    "Atleast for the non raw data. Need to fix this. Probably not hard for data processed into spectrograms since\n",
    "we beforehand know the dimensions. Harder for raw audio segments.\n",
    "\n",
    "Why are we getting MemError though? Could run the DCASE program from home with 16GB RAM.\n",
    "Does not load all features into memory at once? Wonky h5py thing? Check this out!\n",
    "\n",
    "It seems the DCASE code loads all the features into memory.\n",
    "\n",
    "Found a bug, this however does not nessecarily discard the above comments.\n",
    "Working well with memory is still most likely of importance when extracting from large sets.\n",
    "'''\n",
    "class MyF_Ext(FeatureExtractor):\n",
    "    \n",
    "    def __init__(self, config, spectralizer=None):\n",
    "        self.config = config\n",
    "        self.spectralizer = spectralizer\n",
    "        \n",
    "    def extract_features(self):\n",
    "        \n",
    "        if self.config.set.train:\n",
    "            self.extract_train()\n",
    "        else:\n",
    "            self.extract_test()\n",
    "    \n",
    "    '''\n",
    "    Assumes all *.csv and *.wav files are in the same folder which path is in config.\n",
    "    Either creates spectrograms as features or raw audio segments containing events.\n",
    "    Assumes annotations as those provided in \n",
    "    '''\n",
    "    \n",
    "    def extract_train(self):\n",
    "        \n",
    "        \n",
    "        csv_files = [file for file in glob(os.path.join(self.config.path.data_train, '*.csv'))]\n",
    "        \n",
    "        if self.config.features.raw:\n",
    "            \n",
    "            print('Raw extraction')\n",
    "            \n",
    "            events = []\n",
    "            labels = []\n",
    "            \n",
    "            for file in csv_files:\n",
    "            \n",
    "                print('Processing ' + file.replace('csv', 'wav'))\n",
    "                audio, sr = librosa.load(file.replace('csv', 'wav'), self.config.features.sr)\n",
    "                df = pd.read_csv(file, header=0, index_col=False)\n",
    "                df_pos = df[(df == 'POS').any(axis=1)]\n",
    "                \n",
    "                #Add config options for window size around event\n",
    "                df_pos.loc[:, 'Starttime'] = df_pos['Starttime'] - 0.025\n",
    "                df_pos.loc[:, 'Endtime'] = df_pos['Endtime'] + 0.025\n",
    "                start_time = [int(np.floor(start * sr)) for start in df_pos['Starttime']]\n",
    "                end_time = [int(np.floor(end * sr)) for end in df_pos['Endtime']]\n",
    "                \n",
    "                #Better way of doing this?\n",
    "                for i in range(len(start_time)):\n",
    "                    events += [audio[start_time[i]:end_time[i]]]\n",
    "                    \n",
    "                labels += list(chain.from_iterable(\n",
    "                    [df_pos.columns[(df_pos == 'POS').loc[index]].values for index, _ in df_pos.iterrows()]))\n",
    "            \n",
    "            print('Padding')\n",
    "            #Pad arrays in events and format for write\n",
    "            max_len = 0\n",
    "            for e in events:\n",
    "                if len(e) > max_len:\n",
    "                    max_len = len(e)\n",
    "                    \n",
    "            for i in range(len(events)):\n",
    "                if len(events[i]) < max_len:\n",
    "                    events[i] = np.append(events[i], np.array([self.config.features.raw_pad]*(max_len-len(events[i]))))\n",
    "            \n",
    "            events = np.array(events)\n",
    "            \n",
    "            print('Writing to file')\n",
    "            \n",
    "            hf = h5py.File(os.path.join(self.config.path.train_w, 'raw_train.h5'), 'w')\n",
    "            hf.create_dataset('features', data=events)\n",
    "            hf.create_dataset('labels', data=[s.encode() for s in labels], dtype='S20')\n",
    "            hf.close()\n",
    "            \n",
    "            print('Done')\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #DCASE more or less\n",
    "            \n",
    "            print('Spectrogram extraction')\n",
    "            \n",
    "            fps = self.config.features.sr / self.config.features.hop_mel\n",
    "            seg_len = int(round(self.config.features.seg_len * fps))\n",
    "            hop_seg = int(round(self.config.features.hop_seg * fps))\n",
    "            \n",
    "            print(seg_len)\n",
    "            \n",
    "            labels = []\n",
    "            events = []\n",
    "            \n",
    "            for file in csv_files:\n",
    "                \n",
    "                print('Processing ' + file.replace('csv', 'wav'))\n",
    "                audio, sr = librosa.load(file.replace('csv', 'wav'), self.config.features.sr)\n",
    "                \n",
    "                print('Spectral transform')\n",
    "                pcen = self.spectralizer.raw_to_spec(audio, self.config)\n",
    "                print('Done')\n",
    "                \n",
    "                df = pd.read_csv(file, header=0, index_col=False)\n",
    "                df_pos = df[(df == 'POS').any(axis=1)]\n",
    "                \n",
    "                start_time, end_time = time_2_frame(df_pos, fps)\n",
    "                label_f = list(chain.from_iterable(\n",
    "                    [df_pos.columns[(df_pos == 'POS').loc[index]].values for index, _ in df_pos.iterrows()]))\n",
    "                \n",
    "                print('Slicing spectrogram')\n",
    "                \n",
    "                for index in range(len(start_time)):\n",
    "                    \n",
    "                    str_ind = start_time[index]\n",
    "                    end_ind = end_time[index]\n",
    "                    label = label_f[index]\n",
    "                    \n",
    "                    #Event longer than a segment?\n",
    "                    if end_ind - str_ind > seg_len:\n",
    "                        shift = 0\n",
    "                        while end_ind - (str_ind + shift) > seg_len:\n",
    "                            \n",
    "                            pcen_patch = pcen[int(str_ind + shift):int(str_ind + shift + seg_len)]\n",
    "                            events += [pcen_patch]\n",
    "                            labels.append(label)\n",
    "                            shift += hop_seg\n",
    "                        \n",
    "                        pcen_patch = pcen[end_ind - seg_len:end_ind]\n",
    "                        events += [pcen_patch]\n",
    "                        labels.append(label)\n",
    "                    \n",
    "                    #Event shorter than a segment!\n",
    "                    else:\n",
    "                        \n",
    "                        #Repeat the patch til segment length.\n",
    "                        pcen_patch = pcen[str_ind:end_ind]\n",
    "                        if pcen_patch.shape[0] == 0:\n",
    "                            continue\n",
    "                        \n",
    "                        repeats = int(seg_len/(pcen_patch.shape[0])) + 1\n",
    "                        pcen_patch_new = np.tile(pcen_patch, (repeats, 1))\n",
    "                        pcen_patch_new = pcen_patch_new[0:int(seg_len)]\n",
    "                        events += [pcen_patch_new]\n",
    "                        labels.append(label)\n",
    "                        \n",
    "            print('Writing to file')\n",
    "            \n",
    "            print(set([len(e) for e in events]))\n",
    "            \n",
    "            events = np.array(events)\n",
    "            \n",
    "            hf = h5py.File(os.path.join(self.config.path.train_w, 'mel_train.h5'), 'w')\n",
    "            hf.create_dataset('features', data=events)\n",
    "            hf.create_dataset('labels', data=[s.encode() for s in labels], dtype='S20')\n",
    "            hf.close()\n",
    "            \n",
    "            print('Done')\n",
    "                        \n",
    "                \n",
    "                        \n",
    "                \n",
    "    \n",
    "    def extract_test(self, config):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instance with torchlibrosa to be included in model if input is raw.\n",
    "\n",
    "class RawTransformer:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        #Mel stuff etc\n",
    "        self.config = config\n",
    "    \n",
    "    #Input is a training batch?\n",
    "    def rtoi_standard(input):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE 2021 ish\n",
    "#Instance given to DataLoader on argument batch_sampler\n",
    "\n",
    "class RandomEpisodicSampler(data.Sampler):\n",
    "    \n",
    "    #Include the option to choose the number of query samples\n",
    "    #Y_train -> labels, just a list of the targets (list of ints?)\n",
    "    def __init__(self, labels, n_episodes, n_way, n_support, n_query):\n",
    "        \n",
    "        #Number of episodes per epoch. len(labels)/(n_support * n_query) ?\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_way = n_way\n",
    "        self.n_support = n_support\n",
    "        self.n_query = n_query\n",
    "        \n",
    "        self.sample_indices = []\n",
    "        for i in range(max(labels) + 1):\n",
    "            ix = np.argwhere(labels == i).reshape(-1)\n",
    "            ix = torch.from_numpy(ix)\n",
    "            self.sample_indices.append(ix)\n",
    "            \n",
    "        if self.n_way > len(self.samples_indices):\n",
    "            raise ValueError('Error: \"n_way\" parameter is higher than the unique number of classes')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in range(self.n_episodes):\n",
    "            batch = []\n",
    "            #Is not not possible for the same class to be chosen more than once here or am i stupid?\n",
    "            #No we only choose on class once!\n",
    "            classes = torch.randperm(len(self.samples_indices))[:self.n_way]\n",
    "            for c in classes:\n",
    "                #l is a list of indexes of elements in target belonging to class c\n",
    "                l = self.samples_indices[c]\n",
    "                pos = torch.randperm(len(l))[:self.n_samples]\n",
    "                batch.append(l[pos])\n",
    "            batch = torch.stack(batch).t().reshape(-1)\n",
    "            yield batch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveEpisodicSampler(data.Sampler):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config/parse/util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def time_2_frame(df,fps):\n",
    "\n",
    "\n",
    "    #Margin of 25 ms around the onset and offsets\n",
    "    #TODO: Should be in config\n",
    "\n",
    "    df.loc[:,'Starttime'] = df['Starttime'] - 0.025\n",
    "    df.loc[:,'Endtime'] = df['Endtime'] + 0.025\n",
    "\n",
    "    #Converting time to frames\n",
    "\n",
    "    start_time = [int(np.floor(start * fps)) for start in df['Starttime']]\n",
    "\n",
    "    end_time = [int(np.floor(end * fps)) for end in df['Endtime']]\n",
    "\n",
    "    return start_time,end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def class_to_int(labels):\n",
    "    \n",
    "    class_set = set(labels)\n",
    "    ltoix = {label:index for index, label in enumerate(class_set)}\n",
    "    return np.array([ltoix[label] for label in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "#Check over this\n",
    "def balance_class_distribution(X,Y):\n",
    "\n",
    "    '''  Class balancing through Random oversampling\n",
    "    Args:\n",
    "    -X: Feature\n",
    "    -Y: labels\n",
    "\n",
    "    Out:\n",
    "    -X_new: Feature after oversampling\n",
    "    -Y_new: Oversampled label list\n",
    "    '''\n",
    "\n",
    "    x_index = [[index] for index in range(len(X))]\n",
    "    set_y = set(Y)\n",
    "\n",
    "\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    x_unifm, y_unifm = ros.fit_resample(x_index, Y)\n",
    "    unifm_index = [index_new[0] for index_new in x_unifm]\n",
    "\n",
    "    X_new = np.array([X[index] for index in unifm_index])\n",
    "\n",
    "    sampled_index = [idx[0] for idx in x_unifm]\n",
    "    Y_new = np.array([Y[idx] for idx in sampled_index])\n",
    "\n",
    "    return X_new,Y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def norm_params(X):\n",
    "\n",
    "    '''  Normalize features\n",
    "        Args:\n",
    "        - X : Features\n",
    "\n",
    "        Out:\n",
    "        - mean : Mean of the feature set\n",
    "        - std: Standard deviation of the feature set\n",
    "        '''\n",
    "\n",
    "\n",
    "    mean = np.mean(X)\n",
    "\n",
    "    std = np.std(X)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(x, y):\n",
    "    '''\n",
    "    Compute euclidean distance between two tensors\n",
    "    '''\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    if d != y.size(1):\n",
    "        raise Exception\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After loss is in place what more do we need to just start a training loop (No testing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-534a65161f8f>:1: UserWarning: config_path is not specified in hydra.initialize().\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_hydra_main_config_path for more information.\n",
      "  initialize(job_name='test')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize(job_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name='config')\n",
    "\n",
    "s = Spectralizer(cfg)\n",
    "f_ext = MyF_Ext(cfg, s)\n",
    "f_ext.extract_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
