{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import abc\n",
    "import yaml\n",
    "import h5py\n",
    "import librosa\n",
    "import os\n",
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from glob import glob\n",
    "from itertools import chain\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "As of now most of the code in this notebook is more or less copied from the DCASE repository.\n",
    "Minor changes have been done and more is coming to accomodate more flexible FS learning such as\n",
    "active episodic training and most likely more stuff.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHow to make the framework flexible enough that one can point to which samples in a batch are meant to be\\nsupport/query per class? The implementation in DCASE2021 does not handle this.\\n\\n\\nCurrently return the pcen transposed. Where to transpose it back?\\nBatcher? Most important thing is just to not forget i think.\\n\\nThe code only allows one positive class per segment for now I think.\\nThis might be something we would like to fix?\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "How to make the framework flexible enough that one can point to which samples in a batch are meant to be\n",
    "support/query per class? The implementation in DCASE2021 does not handle this.\n",
    "\n",
    "Currently return the pcen transposed. Where to transpose it back?\n",
    "Batcher? Most important thing is just to not forget i think.\n",
    "This doesnt really matter as of now since the model dont care. (Time insensitive)\n",
    "\n",
    "The code only allows one positive class per segment for now I think.\n",
    "This might be something we would like to fix? (How?)\n",
    "Binary applications not uninterestig though\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototypical net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE2021\n",
    "\n",
    "def conv_block(in_channels,out_channels):\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels,out_channels,3,padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE2021\n",
    "\n",
    "#TODO introduce parametrization of conv blocks?\n",
    "class Protonet(nn.Module):\n",
    "    def __init__(self, raw_transformer=None):\n",
    "        super(Protonet,self).__init__()\n",
    "        self.raw_transformer = raw_transformer\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv_block(1,128),\n",
    "            conv_block(128,128),\n",
    "            conv_block(128,128),\n",
    "            conv_block(128,128)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        #Is there risk for this to be super slow?\n",
    "        #A naive approach might transform the same data more than once?\n",
    "        #Lookup tables?\n",
    "        if self.raw_transformer is not None:\n",
    "            x = self.raw_transformer.rtoi_standard(x)\n",
    "        (num_samples,seq_len,mel_bins) = x.shape\n",
    "        x = x.view(-1,1,seq_len,mel_bins)\n",
    "        x = self.encoder(x)\n",
    "        return x.view(x.size(0),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Will most likely lean heavily on the implementation of the DCASE2021 task 5 baseline implementation.\n",
    "\n",
    "'''\n",
    "def prototypical_loss(input, target, n_support, supp_idxs=None):\n",
    "    \n",
    "    target_cpu = target.to('cpu')\n",
    "    input_cpu = input.to('cpu')\n",
    "    classes = torch.unique(target_cpu)\n",
    "    n_classes = len(classes)\n",
    "    n_query = target.eq(classes[0].item()).sum().item() - n_support\n",
    "    if supp_idxs is None:\n",
    "        #Rewrite, need to select only n_support. We might have n_query > n_support\n",
    "        supp_idxs = list(map(lambda c: target_cpu.eq(c).nonzero()[:n_support].squeeze(1), classes))\n",
    "        q_idxs = torch.stack(list(map(lambda c: target_cpu.eq(c).nonzero()[n_support:], classes))).view(-1)\n",
    "    else:\n",
    "        #Work from supp_idxs.\n",
    "        q_idxs = None\n",
    "        \n",
    "    prototypes = torch.stack([input_cpu[idx_list].mean(0) for idx_list in supp_idxs])\n",
    "    query_samples = input_cpu[q_idxs]\n",
    "    #I think prototypes has the wrong dimension here?\n",
    "    #Query samples shape (10,1024)\n",
    "    #Prototypes (2,1,1024)\n",
    "    dists = euclidean_dist(query_samples, prototypes)\n",
    "    \n",
    "    #Check\n",
    "    log_p_y = F.log_softmax(-dists, dim=1).view(n_classes, n_query, -1)\n",
    "    target_inds = torch.arange(0, n_classes)\n",
    "    target_inds = target_inds.view(n_classes, 1, 1)\n",
    "    target_inds = target_inds.expand(n_classes, n_query, 1).long()\n",
    "    #.mean() -> 1/NcNq\n",
    "    loss_val = -log_p_y.gather(2, target_inds).squeeze().view(-1).mean()\n",
    "    _, y_hat = log_p_y.max(2)\n",
    "    acc_val = y_hat.eq(target_inds.squeeze()).float().mean()\n",
    "    return loss_val, acc_val\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    * Design choice: Handle most of pre-processing as part of the model (torchlibrosa)?\\n      May ultimately lead to simpler augmentation etc down the line. Work with raw audio as far as possible?\\n      \\n    * Make use of h5py library for storing training, validation and test sets?\\n      Still raw audio sets?\\n    \\n    * Incorporate pytorch Dataloader, seems prudent and a good design choice.\\n      read(h5py) file + Episodic sampler -> Dataloader?\\n      \\n    * Slight change of mind. Datagen and FeatureExtractor is not really worth spending time on for now.\\n      Sure they could be interfaces for a framework up the road but can do without for now since the loop\\n      will most likely be quite task dependent for now.\\n      \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    * Design choice: Handle most of pre-processing as part of the model (torchlibrosa)?\n",
    "      May ultimately lead to simpler augmentation etc down the line. Work with raw audio as far as possible?\n",
    "      \n",
    "    * Make use of h5py library for storing training, validation and test sets?\n",
    "      Still raw audio sets?\n",
    "    \n",
    "    * Incorporate pytorch Dataloader, seems prudent and a good design choice.\n",
    "      read(h5py) file + Episodic sampler -> Dataloader?\n",
    "      \n",
    "    * Slight change of mind. Datagen and FeatureExtractor is not really worth spending time on for now.\n",
    "      Sure they could be interfaces for a framework up the road but can do without for now since the loop\n",
    "      will most likely be quite task dependent for now.\n",
    "      \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Possibly take a h5 file as input and return X_train, Y_train, X_val, Y_val\n",
    "Is this an approach that we like? Is it commonly used for deep learning?\n",
    "'''\n",
    "\n",
    "#DCASE\n",
    "\n",
    "class Datagen():\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        if config.features.raw:\n",
    "            #These obviosly requires more processing down the pipe but that is application dependent.\n",
    "            #Leave be for now\n",
    "            hf = h5py.File(os.path.join(config.path.train_w, 'raw_train.h5'))\n",
    "        else:\n",
    "            hf = h5py.File(os.path.join(config.path.train_w, 'mel_train.h5'))\n",
    "            self.x = hf['features'][:]\n",
    "            self.labels = [s.decode() for s in hf['labels'][:]]\n",
    "            if config.datagen.ltoi:\n",
    "                self.y = class_to_int(self.labels)\n",
    "            else:\n",
    "                self.y = self.labels\n",
    "            if config.datagen.balance:\n",
    "                self.x, self.y = balance_class_distribution(self.x, self.y)\n",
    "            \n",
    "            array_train = np.arange(len(self.x))\n",
    "            if config.datagen.stratify:\n",
    "                _,_,_,_,train_array,valid_array = train_test_split(self.x, self.y, array_train, \\\n",
    "                                                    random_state=config.datagen.random_state, stratify=self.y)\n",
    "            else:\n",
    "                _,_,_,_,train_array,valid_array = train_test_split(self.x, self.y, array_train, \\\n",
    "                                                    random_state=config.datagen.random_state)\n",
    "                \n",
    "            self.train_index = train_array\n",
    "            self.valid_index = valid_array\n",
    "            if config.datagen.normalize:\n",
    "                self.mean, self.std = norm_params(self.x[train_array])\n",
    "            else:\n",
    "                self.mean = None\n",
    "                self.std = None\n",
    "                \n",
    "    def feature_scale(self, x):\n",
    "        return (x - self.mean)/self.std\n",
    "    \n",
    "    def generate_train(self):\n",
    "        train_array = sorted(self.train_index)\n",
    "        valid_array = sorted(self.valid_index)\n",
    "        X_train = self.x[train_array]\n",
    "        Y_train = self.y[train_array]\n",
    "        X_val = self.x[valid_array]\n",
    "        Y_val = self.y[valid_array]\n",
    "        if self.config.datagen.normalize:\n",
    "            X_train = self.feature_scale(X_train)\n",
    "            X_val = self.feature_scale(X_val)\n",
    "        return X_train, Y_train, X_val, Y_val\n",
    "        \n",
    "\n",
    "#In comparison to parent class instances will work on one particular hfile\n",
    "#and return the relevant datasets, pos, neg, query\n",
    "class TestDatagen(Datagen):\n",
    "    \n",
    "    def __init__(self, hfile, config):\n",
    "        \n",
    "        #Debatable if this should be rewritten in the case where we do not normalize.\n",
    "        #Should really give this some thought overall actually?\n",
    "        #Isnt this normalization somewhat weird?\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.hfile = hfile\n",
    "        \n",
    "    def generate_eval(self):\n",
    "        \n",
    "        X_pos = self.hfile['feat_pos'][:]\n",
    "        X_neg = self.hfile['feat_neg'][:]\n",
    "        X_query = self.hfile['feat_query'][:]\n",
    "        if self.config.datagen.normalize:\n",
    "            X_pos = self.feature_scale(X_pos)\n",
    "            X_neg = self.feature_scale(X_neg)\n",
    "            X_query = self.feature_scale(X_query)\n",
    "            \n",
    "        return X_pos, X_neg, X_query\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This could be an interface / abstract class to build audio \n",
    "to some other format instance to plug into feature extractor\n",
    "'''\n",
    "\n",
    "class Spectralizer():\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "        self.sr = config.features.sr\n",
    "        self.n_fft = config.features.n_fft\n",
    "        self.hop = config.features.hop_mel\n",
    "        self.n_mels = config.features.n_mels\n",
    "        self.fmax = config.features.fmax\n",
    "        \n",
    "\n",
    "    def raw_to_spec(self, audio, config):\n",
    "\n",
    "        #Supposedly suggested by librosa.\n",
    "        audio = audio * (2**32)\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(audio, sr=self.sr, n_fft=self.n_fft, hop_length=self.hop,\n",
    "                                                 n_mels=self.n_mels, fmax=self.fmax)\n",
    "\n",
    "        pcen = librosa.core.pcen(mel_spec, sr=self.sr)\n",
    "        pcen = pcen.astype(np.float32)\n",
    "        \n",
    "        #Note that we transform the features here and therefor have time/frame along dim 0.\n",
    "        #Transform back when loading data? Smaksak\n",
    "        return pcen.T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Possibly work on an raw files and annotations and return/write h5 files.\n",
    "This might be clunky to include in a framework since this most likely is dataset dependent.\n",
    "Might however benfit from having an interface which is inherited by classes working on specific datasets.\n",
    "'''\n",
    "\n",
    "class FeatureExtractor(abc.ABC):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "'''\n",
    "Takes the data from the DCASE (all files one folder) and returns h5 file with the datasets 'features' and 'labels'\n",
    "This takes no heed to unlabeled segments and therefor we will have no unlabeled data to work with.\n",
    "This is an interesting TODO. Most likely need to rework some of the mechanisms here to work with limited RAM.\n",
    "Extract segment -> write to file etc... Look at DCASE code for example\n",
    "Unlabeled data could be saved to a new dataset 'unlabeled' for example.\n",
    "\n",
    "\n",
    "TODO: MemError already present even before processing unlabeled data and only one of the smaller audio files.\n",
    "Atleast for the non raw data. Need to fix this. Probably not hard for data processed into spectrograms since\n",
    "we beforehand know the dimensions. Harder for raw audio segments.\n",
    "\n",
    "Why are we getting MemError though? Could run the DCASE program from home with 16GB RAM.\n",
    "Does not load all features into memory at once? Wonky h5py thing? Check this out!\n",
    "\n",
    "It seems the DCASE code loads all the features into memory.\n",
    "\n",
    "Found a bug, this however does not nessecarily discard the above comments.\n",
    "Working well with memory is still most likely of importance when extracting from large sets.\n",
    "'''\n",
    "class MyF_Ext(FeatureExtractor):\n",
    "    \n",
    "    def __init__(self, config, spectralizer=None):\n",
    "        self.config = config\n",
    "        self.spectralizer = spectralizer\n",
    "        \n",
    "    def extract_features(self):\n",
    "        \n",
    "        self.extract_train()\n",
    "        self.extract_test()\n",
    "    \n",
    "    '''\n",
    "    Assumes all *.csv and *.wav files are in the same folder which path is in config.\n",
    "    Either creates spectrograms as features or raw audio segments containing events.\n",
    "    Assumes annotations as those provided in \n",
    "    '''\n",
    "    \n",
    "    def extract_train(self):\n",
    "        \n",
    "        print('--- Processing training data ---')\n",
    "        csv_files = [file for file in glob(os.path.join(self.config.path.data_train, '*.csv'))]\n",
    "        \n",
    "        if self.config.features.raw:\n",
    "            \n",
    "            print('Raw extraction')\n",
    "            \n",
    "            events = []\n",
    "            labels = []\n",
    "            \n",
    "            for file in csv_files:\n",
    "            \n",
    "                print('Processing ' + file.replace('csv', 'wav'))\n",
    "                audio, sr = librosa.load(file.replace('csv', 'wav'), self.config.features.sr)\n",
    "                df = pd.read_csv(file, header=0, index_col=False)\n",
    "                df_pos = df[(df == 'POS').any(axis=1)]\n",
    "                \n",
    "                #Add config options for window size around event\n",
    "                df_pos.loc[:, 'Starttime'] = df_pos['Starttime'] - 0.025\n",
    "                df_pos.loc[:, 'Endtime'] = df_pos['Endtime'] + 0.025\n",
    "                start_time = [int(np.floor(start * sr)) for start in df_pos['Starttime']]\n",
    "                end_time = [int(np.floor(end * sr)) for end in df_pos['Endtime']]\n",
    "                \n",
    "                #Better way of doing this?\n",
    "                for i in range(len(start_time)):\n",
    "                    events += [audio[start_time[i]:end_time[i]]]\n",
    "                    \n",
    "                labels += list(chain.from_iterable(\n",
    "                    [df_pos.columns[(df_pos == 'POS').loc[index]].values for index, _ in df_pos.iterrows()]))\n",
    "            \n",
    "            print('Padding')\n",
    "            #Pad arrays in events and format for write\n",
    "            max_len = 0\n",
    "            for e in events:\n",
    "                if len(e) > max_len:\n",
    "                    max_len = len(e)\n",
    "                    \n",
    "            for i in range(len(events)):\n",
    "                if len(events[i]) < max_len:\n",
    "                    events[i] = np.append(events[i], np.array([self.config.features.raw_pad]*(max_len-len(events[i]))))\n",
    "            \n",
    "            events = np.array(events)\n",
    "            \n",
    "            print('Writing to file')\n",
    "            \n",
    "            hf = h5py.File(os.path.join(self.config.path.train_w, 'raw_train.h5'), 'w')\n",
    "            hf.create_dataset('features', data=events)\n",
    "            hf.create_dataset('labels', data=[s.encode() for s in labels], dtype='S20')\n",
    "            hf.close()\n",
    "            \n",
    "            print('Done')\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #DCASE more or less\n",
    "            \n",
    "            print('Spectrogram extraction')\n",
    "            \n",
    "            fps = self.config.features.sr / self.config.features.hop_mel\n",
    "            seg_len = int(round(self.config.features.seg_len * fps))\n",
    "            hop_seg = int(round(self.config.features.hop_seg * fps))\n",
    "            \n",
    "            labels = []\n",
    "            events = []\n",
    "            \n",
    "            for file in csv_files:\n",
    "                \n",
    "                print('Processing ' + file.replace('csv', 'wav'))\n",
    "                audio, sr = librosa.load(file.replace('csv', 'wav'), self.config.features.sr)\n",
    "                \n",
    "                print('Spectral transform')\n",
    "                pcen = self.spectralizer.raw_to_spec(audio, self.config)\n",
    "                \n",
    "                df = pd.read_csv(file, header=0, index_col=False)\n",
    "                df_pos = df[(df == 'POS').any(axis=1)]\n",
    "                \n",
    "                start_time, end_time = time_2_frame(df_pos, fps)\n",
    "                label_f = list(chain.from_iterable(\n",
    "                    [df_pos.columns[(df_pos == 'POS').loc[index]].values for index, _ in df_pos.iterrows()]))\n",
    "                \n",
    "                print('Slicing spectrogram')\n",
    "                \n",
    "                for index in range(len(start_time)):\n",
    "                    \n",
    "                    str_ind = start_time[index]\n",
    "                    end_ind = end_time[index]\n",
    "                    label = label_f[index]\n",
    "                    \n",
    "                    #Event longer than a segment?\n",
    "                    if end_ind - str_ind > seg_len:\n",
    "                        shift = 0\n",
    "                        while end_ind - (str_ind + shift) > seg_len:\n",
    "                            \n",
    "                            pcen_patch = pcen[int(str_ind + shift):int(str_ind + shift + seg_len)]\n",
    "                            events += [pcen_patch]\n",
    "                            labels.append(label)\n",
    "                            shift += hop_seg\n",
    "                        \n",
    "                        pcen_patch = pcen[end_ind - seg_len:end_ind]\n",
    "                        events += [pcen_patch]\n",
    "                        labels.append(label)\n",
    "                    \n",
    "                    #Event shorter than a segment!\n",
    "                    else:\n",
    "                        \n",
    "                        #Repeat the patch til segment length.\n",
    "                        pcen_patch = pcen[str_ind:end_ind]\n",
    "                        if pcen_patch.shape[0] == 0:\n",
    "                            continue\n",
    "                        \n",
    "                        repeats = int(seg_len/(pcen_patch.shape[0])) + 1\n",
    "                        pcen_patch_new = np.tile(pcen_patch, (repeats, 1))\n",
    "                        pcen_patch_new = pcen_patch_new[0:int(seg_len)]\n",
    "                        events += [pcen_patch_new]\n",
    "                        labels.append(label)\n",
    "                        \n",
    "            print('Writing to file')\n",
    "            \n",
    "            events = np.array(events)\n",
    "            \n",
    "            hf = h5py.File(os.path.join(self.config.path.train_w, 'mel_train.h5'), 'w')\n",
    "            hf.create_dataset('features', data=events)\n",
    "            hf.create_dataset('labels', data=[s.encode() for s in labels], dtype='S20')\n",
    "            hf.close()\n",
    "            \n",
    "            print('Done')\n",
    "                        \n",
    "                \n",
    "                        \n",
    "                \n",
    "    #Try to start out in a way that would make it easier to possibly incorporate multiple negative classes\n",
    "    #down the line. This needs to be reflected in TestDatagen. Perhaps just list of lists with indexes to \n",
    "    #the h5 dataset 'feat_neg'. Then if this is not in keys just assume that only one negative class exists.\n",
    "    \n",
    "    #For now just a copy of the DCASE code, this since the way they work here most likely have an impact on the\n",
    "    #scoring/evaluation metrics on the github.\n",
    "    def extract_test(self):\n",
    "        \n",
    "        print('--- Processing test data ---')\n",
    "        csv_files = [file for file in glob(os.path.join(self.config.path.data_test, '*.csv'))]\n",
    "        \n",
    "        #Are we ever interested in a raw extraction here?\n",
    "        if self.config.features.raw:\n",
    "            pass\n",
    "        else:\n",
    "            \n",
    "            fps = self.config.features.sr / self.config.features.hop_mel\n",
    "            seg_len = int(round(self.config.features.seg_len * fps))\n",
    "            hop_seg = int(round(self.config.features.hop_seg * fps))\n",
    "            \n",
    "            for file in csv_files:\n",
    "                \n",
    "                print('Processing ' + file.replace('csv', 'wav'))\n",
    "                \n",
    "                idx_pos = 0\n",
    "                idx_neg = 0\n",
    "                start_neg = 0\n",
    "                hop_neg = 0\n",
    "                idx_query = 0\n",
    "                hop_query = 0\n",
    "                strt_index = 0\n",
    "\n",
    "                split_list = file.split('/')\n",
    "                name = str(split_list[-1].split('.')[0])\n",
    "                feat_name = name + '.h5'\n",
    "                audio_path = file.replace('csv', 'wav')\n",
    "                feat_info = []\n",
    "                hdf_eval = os.path.join(self.config.path.test_w ,feat_name)\n",
    "                hf = h5py.File(hdf_eval,'w')\n",
    "                hf.create_dataset('feat_pos', shape=(0, seg_len, self.config.features.n_mels),\n",
    "                                  maxshape= (None, seg_len, self.config.features.n_mels))\n",
    "                hf.create_dataset('feat_query',shape=(0,seg_len, self.config.features.n_mels),maxshape=(None,seg_len,self.config.features.n_mels))\n",
    "                hf.create_dataset('feat_neg',shape=(0,seg_len, self.config.features.n_mels),maxshape=(None,seg_len,self.config.features.n_mels))\n",
    "                hf.create_dataset('start_index_query',shape=(1,),maxshape=(None))\n",
    "\n",
    "                'In case you want to use the statistics of each file to normalize'\n",
    "\n",
    "                hf.create_dataset('mean_global',shape=(1,), maxshape=(None))\n",
    "                hf.create_dataset('std_dev_global',shape=(1,), maxshape=(None))\n",
    "\n",
    "                df_eval = pd.read_csv(file, header=0, index_col=False)\n",
    "                Q_list = df_eval['Q'].to_numpy()\n",
    "\n",
    "                start_time,end_time = time_2_frame(df_eval,fps)\n",
    "\n",
    "                index_sup = np.where(Q_list == 'POS')[0][:self.config.train.n_shot]\n",
    "\n",
    "                audio, sr = librosa.load(file.replace('csv', 'wav'), self.config.features.sr)\n",
    "                print('Spectral transform')\n",
    "                pcen = self.spectralizer.raw_to_spec(audio, self.config)\n",
    "               \n",
    "                mean = np.mean(pcen)\n",
    "                std = np.mean(pcen)\n",
    "                hf['mean_global'][:] = mean\n",
    "                hf['std_dev_global'][:] = std\n",
    "\n",
    "                strt_indx_query = end_time[index_sup[-1]]\n",
    "                end_idx_neg = pcen.shape[0] - 1\n",
    "                hf['start_index_query'][:] = strt_indx_query\n",
    "\n",
    "                print(\"Creating negative dataset\")\n",
    "\n",
    "                while end_idx_neg - (strt_index + hop_neg) > seg_len:\n",
    "\n",
    "                    patch_neg = pcen[int(strt_index + hop_neg):int(strt_index + hop_neg + seg_len)]\n",
    "\n",
    "                    hf['feat_neg'].resize((idx_neg + 1, patch_neg.shape[0], patch_neg.shape[1]))\n",
    "                    hf['feat_neg'][idx_neg] = patch_neg\n",
    "                    idx_neg += 1\n",
    "                    hop_neg += hop_seg\n",
    "\n",
    "                last_patch = pcen[end_idx_neg - seg_len:end_idx_neg]\n",
    "                hf['feat_neg'].resize((idx_neg + 1, last_patch.shape[0], last_patch.shape[1]))\n",
    "                hf['feat_neg'][idx_neg] = last_patch\n",
    "\n",
    "                print(\"Creating Positive dataset\")\n",
    "                for index in index_sup:\n",
    "\n",
    "                    str_ind = int(start_time[index])\n",
    "                    end_ind = int(end_time[index])\n",
    "\n",
    "                    if end_ind - str_ind > seg_len:\n",
    "\n",
    "                        shift = 0\n",
    "                        while end_ind - (str_ind + shift) > seg_len:\n",
    "\n",
    "                            patch_pos = pcen[int(str_ind + shift):int(str_ind + shift + seg_len)]\n",
    "\n",
    "                            hf['feat_pos'].resize((idx_pos + 1, patch_pos.shape[0], patch_pos.shape[1]))\n",
    "                            hf['feat_pos'][idx_pos] = patch_pos\n",
    "                            idx_pos += 1\n",
    "                            shift += hop_seg\n",
    "                        last_patch_pos = pcen[end_ind - seg_len:end_ind]\n",
    "                        hf['feat_pos'].resize((idx_pos + 1, patch_pos.shape[0], patch_pos.shape[1]))\n",
    "                        hf['feat_pos'][idx_pos] = last_patch_pos\n",
    "                        idx_pos += 1\n",
    "\n",
    "                    else:\n",
    "                        patch_pos = pcen[str_ind:end_ind]\n",
    "\n",
    "                        if patch_pos.shape[0] == 0:\n",
    "                            print(patch_pos.shape[0])\n",
    "                            print(\"The patch is of 0 length\")\n",
    "                            continue\n",
    "                        repeat_num = int(seg_len / (patch_pos.shape[0])) + 1\n",
    "\n",
    "                        patch_new = np.tile(patch_pos, (repeat_num, 1))\n",
    "                        patch_new = patch_new[0:int(seg_len)]\n",
    "                        hf['feat_pos'].resize((idx_pos + 1, patch_new.shape[0], patch_new.shape[1]))\n",
    "                        hf['feat_pos'][idx_pos] = patch_new\n",
    "                        idx_pos += 1\n",
    "\n",
    "\n",
    "\n",
    "                print(\"Creating query dataset\")\n",
    "\n",
    "                while end_idx_neg - (strt_indx_query + hop_query) > seg_len:\n",
    "\n",
    "                    patch_query = pcen[int(strt_indx_query + hop_query):int(strt_indx_query + hop_query + seg_len)]\n",
    "                    hf['feat_query'].resize((idx_query + 1, patch_query.shape[0], patch_query.shape[1]))\n",
    "                    hf['feat_query'][idx_query] = patch_query\n",
    "                    idx_query += 1\n",
    "                    hop_query += hop_seg\n",
    "\n",
    "\n",
    "                last_patch_query = pcen[end_idx_neg - seg_len:end_idx_neg]\n",
    "\n",
    "                hf['feat_query'].resize((idx_query + 1, last_patch_query.shape[0], last_patch_query.shape[1]))\n",
    "                hf['feat_query'][idx_query] = last_patch_query\n",
    "\n",
    "                hf.close()\n",
    "\n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instance with torchlibrosa to be included in model if input is raw.\n",
    "#Having seconds thaughts on putting the data raw into the models.\n",
    "#Extracting raw features and DataGenning them still of interest.\n",
    "#But transform dataset before training?\n",
    "\n",
    "class RawTransformer:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        #Mel stuff etc\n",
    "        self.config = config\n",
    "    \n",
    "    #Input is a training batch?\n",
    "    def rtoi_standard(input):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE 2021 ish\n",
    "#Instance given to DataLoader on argument batch_sampler\n",
    "\n",
    "class RandomEpisodicSampler(data.Sampler):\n",
    "    \n",
    "    #Include the option to choose the number of query samples\n",
    "    #Y_train -> labels, just a list of the targets (list of ints?)\n",
    "    def __init__(self, labels, n_episodes, n_way, n_support, n_query):\n",
    "        \n",
    "        #Number of episodes per epoch. len(labels)/(n_support * n_query) ?\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_way = n_way\n",
    "        self.n_support = n_support\n",
    "        self.n_query = n_query\n",
    "        self.n_samples = n_support+n_query\n",
    "        \n",
    "        labels = np.array(labels)\n",
    "        self.sample_indices = []\n",
    "        for i in range(max(labels) + 1):\n",
    "            ix = np.argwhere(labels == i).reshape(-1)\n",
    "            ix = torch.from_numpy(ix)\n",
    "            self.sample_indices.append(ix)\n",
    "            \n",
    "        if self.n_way > len(self.sample_indices):\n",
    "            #print(self.n_way)\n",
    "            raise ValueError('Error: \"n_way\" parameter is higher than the unique number of classes')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in range(self.n_episodes):\n",
    "            batch = []\n",
    "            classes = torch.randperm(len(self.sample_indices))[:self.n_way]\n",
    "            for c in classes:\n",
    "                #l is a list of indexes of elements in target belonging to class c\n",
    "                l = self.sample_indices[c]\n",
    "                pos = torch.randperm(len(l))[:self.n_samples]\n",
    "                batch.append(l[pos])\n",
    "            batch = torch.stack(batch).t().reshape(-1)\n",
    "            yield batch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Must somehow have access to all the data (just pass it).\n",
    "\n",
    "class ActiveEpisodicSampler(data.Sampler):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util/Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def time_2_frame(df,fps):\n",
    "\n",
    "\n",
    "    #Margin of 25 ms around the onset and offsets\n",
    "    #TODO: Should be in config\n",
    "\n",
    "    df.loc[:,'Starttime'] = df['Starttime'] - 0.025\n",
    "    df.loc[:,'Endtime'] = df['Endtime'] + 0.025\n",
    "\n",
    "    #Converting time to frames\n",
    "\n",
    "    start_time = [int(np.floor(start * fps)) for start in df['Starttime']]\n",
    "\n",
    "    end_time = [int(np.floor(end * fps)) for end in df['Endtime']]\n",
    "\n",
    "    return start_time,end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def class_to_int(labels):\n",
    "    \n",
    "    class_set = set(labels)\n",
    "    ltoix = {label:index for index, label in enumerate(class_set)}\n",
    "    return np.array([ltoix[label] for label in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "#Check over this\n",
    "def balance_class_distribution(X,Y):\n",
    "\n",
    "    '''  Class balancing through Random oversampling\n",
    "    Args:\n",
    "    -X: Feature\n",
    "    -Y: labels\n",
    "\n",
    "    Out:\n",
    "    -X_new: Feature after oversampling\n",
    "    -Y_new: Oversampled label list\n",
    "    '''\n",
    "\n",
    "    x_index = [[index] for index in range(len(X))]\n",
    "    set_y = set(Y)\n",
    "\n",
    "\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    x_unifm, y_unifm = ros.fit_resample(x_index, Y)\n",
    "    unifm_index = [index_new[0] for index_new in x_unifm]\n",
    "\n",
    "    X_new = np.array([X[index] for index in unifm_index])\n",
    "\n",
    "    sampled_index = [idx[0] for idx in x_unifm]\n",
    "    Y_new = np.array([Y[idx] for idx in sampled_index])\n",
    "\n",
    "    return X_new,Y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def norm_params(X):\n",
    "\n",
    "    '''  Normalize features\n",
    "        Args:\n",
    "        - X : Features\n",
    "\n",
    "        Out:\n",
    "        - mean : Mean of the feature set\n",
    "        - std: Standard deviation of the feature set\n",
    "        '''\n",
    "\n",
    "\n",
    "    mean = np.mean(X)\n",
    "\n",
    "    std = np.std(X)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def euclidean_dist(x, y):\n",
    "    '''\n",
    "    Compute euclidean distance between two tensors\n",
    "    '''\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    if d != y.size(1):\n",
    "        #we are currently getting stuck here?\n",
    "        #why?\n",
    "        raise Exception\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def balance_class_distribution(X,Y):\n",
    "\n",
    "    '''  Class balancing through Random oversampling\n",
    "    Args:\n",
    "    -X: Feature\n",
    "    -Y: labels\n",
    "\n",
    "    Out:\n",
    "    -X_new: Feature after oversampling\n",
    "    -Y_new: Oversampled label list\n",
    "    '''\n",
    "\n",
    "    x_index = [[index] for index in range(len(X))]\n",
    "    set_y = set(Y)\n",
    "\n",
    "\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    x_unifm, y_unifm = ros.fit_resample(x_index, Y)\n",
    "    unifm_index = [index_new[0] for index_new in x_unifm]\n",
    "\n",
    "    X_new = np.array([X[index] for index in unifm_index])\n",
    "\n",
    "    sampled_index = [idx[0] for idx in x_unifm]\n",
    "    Y_new = np.array([Y[idx] for idx in sampled_index])\n",
    "\n",
    "    return X_new,Y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "#TODO Make it possible to use all samples as negatives.\n",
    "#TODO Read up on this function, understand it better.\n",
    "\n",
    "\n",
    "def evaluate_prototypes(conf=None,hdf_eval=None,device= None,strt_index_query=None):\n",
    "\n",
    "    \"\"\" Run the evaluation\n",
    "    Args:\n",
    "     - conf: config object\n",
    "     - hdf_eval: Features from the audio file\n",
    "     - device:  cuda/cpu\n",
    "     - str_index_query : start frame of the query set w.r.t to the original file\n",
    "\n",
    "     Out:\n",
    "     - onset: Onset array predicted by the model\n",
    "     - offset: Offset array predicted by the model\n",
    "      \"\"\"\n",
    "    hop_seg = int(conf.features.hop_seg * conf.features.sr // conf.features.hop_mel)\n",
    "\n",
    "    gen_eval = TestDatagen(hdf_eval,conf)\n",
    "    X_pos, X_neg,X_query = gen_eval.generate_eval()\n",
    "\n",
    "    X_pos = torch.tensor(X_pos)\n",
    "    Y_pos = torch.LongTensor(np.zeros(X_pos.shape[0]))\n",
    "    X_neg = torch.tensor(X_neg)\n",
    "    Y_neg = torch.LongTensor(np.zeros(X_neg.shape[0]))\n",
    "    X_query = torch.tensor(X_query)\n",
    "    Y_query = torch.LongTensor(np.zeros(X_query.shape[0]))\n",
    "\n",
    "    num_batch_query = len(Y_query) // conf.eval.query_batch_size\n",
    "\n",
    "    query_dataset = torch.utils.data.TensorDataset(X_query, Y_query)\n",
    "    q_loader = torch.utils.data.DataLoader(dataset=query_dataset, batch_sampler=None,batch_size=conf.eval.query_batch_size,shuffle=False)\n",
    "    query_set_feat = torch.zeros(0,1024).cpu()\n",
    "\n",
    "\n",
    "    Model = Protonet()\n",
    "\n",
    "    if device == 'cpu':\n",
    "        Model.load_state_dict(torch.load(conf.path.best_model, map_location=torch.device('cpu')))\n",
    "    else:\n",
    "        Model.load_state_dict(torch.load(conf.path.best_model))\n",
    "\n",
    "    Model.to(device)\n",
    "    Model.eval()\n",
    "\n",
    "    'List for storing the combined probability across all iterations'\n",
    "    prob_comb = []\n",
    "\n",
    "    iterations = conf.eval.iterations\n",
    "    for i in range(iterations):\n",
    "        prob_pos_iter = []\n",
    "        neg_indices = torch.randperm(len(X_neg))[:conf.eval.samples_neg]\n",
    "        X_neg = X_neg[neg_indices]\n",
    "        Y_neg = Y_neg[neg_indices]\n",
    "        batch_size_neg = conf.eval.negative_set_batch_size\n",
    "        neg_dataset = torch.utils.data.TensorDataset(X_neg, Y_neg)\n",
    "        negative_loader = torch.utils.data.DataLoader(dataset=neg_dataset, batch_sampler=None, batch_size=batch_size_neg)\n",
    "\n",
    "        batch_samplr_pos = RandomEpisodicSampler(Y_pos, num_batch_query + 1, 1, conf.train.n_shot, conf.train.n_query)\n",
    "        pos_dataset = torch.utils.data.TensorDataset(X_pos, Y_pos)\n",
    "        pos_loader = torch.utils.data.DataLoader(dataset=pos_dataset, batch_sampler=batch_samplr_pos)\n",
    "\n",
    "        neg_iterator = iter(negative_loader)\n",
    "        pos_iterator = iter(pos_loader)\n",
    "        q_iterator = iter(q_loader)\n",
    "\n",
    "        print(\"Iteration number {}\".format(i))\n",
    "\n",
    "        for batch in tqdm(neg_iterator):\n",
    "            x_neg, y_neg = batch\n",
    "            x_neg = x_neg.to(device)\n",
    "            feat_neg = Model(x_neg)\n",
    "            feat_neg = feat_neg.detach().cpu()\n",
    "            query_set_feat = torch.cat((query_set_feat, feat_neg), dim=0)\n",
    "        neg_proto = query_set_feat.mean(dim=0)\n",
    "        neg_proto =neg_proto.to(device)\n",
    "\n",
    "        for batch in tqdm(q_iterator):\n",
    "            x_q, y_q = batch\n",
    "            x_q = x_q.to(device)\n",
    "            #Why even bother with a data loader for the positive class?\n",
    "            #Are we not only drawing the 5 sample that there is repeatedly?\n",
    "            #Could just run the positives through the network once and save the\n",
    "            #Prototype. Check that I am right about this.\n",
    "            x_pos, y_pos = next(pos_iterator)\n",
    "            x_pos = x_pos.to(device)\n",
    "            x_pos = Model(x_pos)\n",
    "            x_query = Model(x_q)\n",
    "            probability_pos = get_probability(x_pos, neg_proto, x_query)\n",
    "            prob_pos_iter.extend(probability_pos)\n",
    "\n",
    "        prob_comb.append(prob_pos_iter)\n",
    "\n",
    "    prob_final = np.mean(np.array(prob_comb),axis=0)\n",
    "\n",
    "    krn = np.array([1, -1])\n",
    "    prob_thresh = np.where(prob_final > conf.eval.p_thresh, 1, 0)\n",
    "\n",
    "    prob_pos_final = prob_final * prob_thresh\n",
    "    changes = np.convolve(krn, prob_thresh)\n",
    "\n",
    "    onset_frames = np.where(changes == 1)[0]\n",
    "    offset_frames = np.where(changes == -1)[0]\n",
    "\n",
    "    str_time_query = strt_index_query * conf.features.hop_mel / conf.features.sr\n",
    "\n",
    "    onset = (onset_frames + 1) * (hop_seg) * conf.features.hop_mel / conf.features.sr\n",
    "    onset = onset + str_time_query\n",
    "\n",
    "    offset = (offset_frames + 1) * (hop_seg) * conf.features.hop_mel / conf.features.sr\n",
    "    offset = offset + str_time_query\n",
    "\n",
    "    assert len(onset) == len(offset)\n",
    "    return onset, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def get_probability(x_pos,neg_proto,query_set_out):\n",
    "\n",
    "\n",
    "    \"\"\"Calculates the  probability of each query point belonging to either the positive or negative class\n",
    "     Args:\n",
    "     - x_pos : Model output for the positive class\n",
    "     - neg_proto : Negative class prototype calculated from randomly chosed 100 segments across the audio file\n",
    "     - query_set_out:  Model output for the first 8 samples of the query set\n",
    "\n",
    "     Out:\n",
    "     - Probabiility array for the positive class\n",
    "     \"\"\"\n",
    "\n",
    "    pos_prototype = x_pos.mean(0)\n",
    "    prototypes = torch.stack([pos_prototype,neg_proto])\n",
    "    dists = euclidean_dist(query_set_out,prototypes)\n",
    "    '''  Taking inverse distance for converting distance to probabilities'''\n",
    "    inverse_dist = torch.div(1.0, dists)\n",
    "    prob = torch.softmax(inverse_dist,dim=1)\n",
    "    '''  Probability array for positive class'''\n",
    "    prob_pos = prob[:,0]\n",
    "\n",
    "    return prob_pos.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, config, num_batches_tr, num_batches_val):\n",
    "    \n",
    "    if config.set.device == 'cuda':\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    #Should this be done here or passed into this function?\n",
    "    #Could be configs for more terminal flexibility\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=config.train.lr_rate)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optim, gamma=config.train.scheduler_gamma,\n",
    "                                                  step_size=config.train.scheduler_step_size)\n",
    "    num_epochs = config.train.epochs\n",
    "    \n",
    "    best_model_path = config.path.best_model\n",
    "    last_model_path = config.path.last_model\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    best_val_acc = 0.0\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print('Epoch {}'.format(epoch))\n",
    "        train_iterator = iter(train_loader)\n",
    "        for batch in tqdm(train_iterator):\n",
    "            optim.zero_grad()\n",
    "            model.train()\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            x_out = model(x)\n",
    "            tr_loss, tr_acc = prototypical_loss(x_out, y, config.train.n_shot)\n",
    "            train_loss.append(tr_loss.item())\n",
    "            train_acc.append(tr_acc.item())\n",
    "            \n",
    "            tr_loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "        avg_loss_tr = np.mean(train_loss[-num_batches_tr:])\n",
    "        avg_acc_tr = np.mean(train_acc[-num_batches_tr:])\n",
    "        print('Average train loss: {}  Average training accuracy: {}'.format(avg_loss_tr,avg_acc_tr))\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        #No dropouts in model for now, I think there is no difference between train and eval mode\n",
    "        model.eval()\n",
    "        val_iterator = iter(val_loader)\n",
    "        for batch in tqdm(val_iterator):\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            x_val = model(x)\n",
    "            valid_loss, valid_acc = prototypical_loss(x_val, y, config.train.n_shot)\n",
    "            val_loss.append(valid_loss.item())\n",
    "            val_acc.append(valid_acc.item())\n",
    "        avg_loss_val = np.mean(val_loss[-num_batches_val:])\n",
    "        avg_acc_val = np.mean(val_acc[-num_batches_val:])\n",
    "        \n",
    "        print ('Epoch {}, Validation loss {:.4f}, Validation accuracy {:.4f}'.format(epoch,avg_loss_val,avg_acc_val))\n",
    "        if avg_acc_val > best_val_acc:\n",
    "            print(\"Saving the best model with valdation accuracy {}\".format(avg_acc_val))\n",
    "            best_val_acc = avg_acc_val\n",
    "            best_state = model.state_dict()\n",
    "            torch.save(model.state_dict(),best_model_path)\n",
    "    torch.save(model.state_dict(),last_model_path)\n",
    "\n",
    "    return best_val_acc, model, best_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE\n",
    "\n",
    "def eval(config):\n",
    "    \n",
    "    if config.set.device == 'cuda':\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "    name_arr = np.array([])\n",
    "    onset_arr = np.array([])\n",
    "    offset_arr = np.array([])\n",
    "    all_feat_files = [file for file in glob(os.path.join(config.path.test_w,'*.h5'))]\n",
    "\n",
    "    for feat_file in all_feat_files:\n",
    "        feat_name = feat_file.split('/')[-1]\n",
    "        audio_name = feat_name.replace('h5','wav')\n",
    "\n",
    "        print(\"Processing audio file : {}\".format(audio_name))\n",
    "\n",
    "        hdf_eval = h5py.File(feat_file,'r')\n",
    "        strt_index_query =  hdf_eval['start_index_query'][:][0]\n",
    "        onset,offset = evaluate_prototypes(config, hdf_eval, device, strt_index_query)\n",
    "\n",
    "        name = np.repeat(audio_name,len(onset))\n",
    "        name_arr = np.append(name_arr,name)\n",
    "        onset_arr = np.append(onset_arr,onset)\n",
    "        offset_arr = np.append(offset_arr,offset)\n",
    "\n",
    "    df_out = pd.DataFrame({'Audiofilename':name_arr,'Starttime':onset_arr,'Endtime':offset_arr})\n",
    "    csv_path = os.path.join(config.path.root,'Eval_out.csv')\n",
    "    df_out.to_csv(csv_path,index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19549/195687565.py:1: UserWarning: config_path is not specified in hydra.initialize().\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_hydra_main_config_path for more information.\n",
      "  initialize(job_name='test')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize(job_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name='config')\n",
    "s = Spectralizer(cfg)\n",
    "f_ext = MyF_Ext(cfg, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f_ext.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing test data ---\n",
      "Processing /home/martin/data/dcase/small_test/a1.wav\n",
      "Spectral transform\n",
      "Creating negative dataset\n",
      "Creating Positive dataset\n",
      "Creating query dataset\n"
     ]
    }
   ],
   "source": [
    "f_ext.extract_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = Datagen(cfg)\n",
    "X_train, Y_train, X_val, Y_val = data_gen.generate_train()\n",
    "X_tr = torch.tensor(X_train)\n",
    "Y_tr = torch.LongTensor(Y_train)\n",
    "X_val = torch.tensor(X_val)\n",
    "Y_val = torch.LongTensor(Y_val)\n",
    "samples_per_cls = cfg.train.n_shot + cfg.train.n_query\n",
    "batch_size_tr = samples_per_cls * cfg.train.k_way\n",
    "batch_size_vd = batch_size_tr\n",
    "\n",
    "num_batches_tr = len(Y_train)//batch_size_tr\n",
    "num_batches_vd = len(Y_val)//batch_size_vd\n",
    "\n",
    "\n",
    "samplr_train = RandomEpisodicSampler(Y_train,num_batches_tr,cfg.train.k_way, cfg.train.n_shot, cfg.train.n_query)\n",
    "samplr_valid = RandomEpisodicSampler(Y_val,num_batches_vd,cfg.train.k_way,cfg.train.n_shot, cfg.train.n_query)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_tr,Y_tr)\n",
    "valid_dataset = torch.utils.data.TensorDataset(X_val,Y_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_sampler=samplr_train,num_workers=0,pin_memory=True,shuffle=False)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,batch_sampler=samplr_valid,num_workers=0,pin_memory=True,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|                              | 114/444 [00:14<00:43,  7.65it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19549/2980947593.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProtonet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_batches_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_batches_vd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best accuracy of the model on training set is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19549/4284238991.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, config, num_batches_tr, num_batches_val)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mtr_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mavg_loss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnum_batches_tr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    108\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Protonet()\n",
    "best_acc,model,best_state = train(model,train_loader,valid_loader,cfg,num_batches_tr,num_batches_vd)\n",
    "print(\"Best accuracy of the model on training set is {}\".format(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio file : a1.wav\n",
      "Iteration number 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 41/41 [00:01<00:00, 29.86it/s]\n",
      "  9%|                                    | 784/8822 [00:30<05:08, 26.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19549/3135226110.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_19549/2280049708.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mhdf_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mstrt_index_query\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mhdf_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_index_query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0monset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_prototypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdf_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrt_index_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19549/4150131940.py\u001b[0m in \u001b[0;36mevaluate_prototypes\u001b[0;34m(conf, hdf_eval, device, strt_index_query)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mx_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mx_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mx_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mprobability_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_proto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mprob_pos_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19549/1533042516.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmel_bins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmel_bins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
