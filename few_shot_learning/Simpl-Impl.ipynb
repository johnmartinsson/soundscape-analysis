{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import abc\n",
    "import yaml\n",
    "import h5py\n",
    "import librosa\n",
    "import os\n",
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from glob import glob\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHow to make the framework flexible enough that one can point to which samples in a batch are meant to be\\nsupport/query per class? The implementation in DCASE2021 does not handle this.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "How to make the framework flexible enough that one can point to which samples in a batch are meant to be\n",
    "support/query per class? The implementation in DCASE2021 does not handle this.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototypical net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE2021\n",
    "\n",
    "def conv_block(in_channels,out_channels):\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels,out_channels,3,padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE2021\n",
    "\n",
    "#TODO introduce parametrization of conv blocks?\n",
    "class Protonet(nn.Module):\n",
    "    def __init__(self, raw_transformer=None):\n",
    "        super(Protonet,self).__init__()\n",
    "        self.raw_transformer = raw_transformer\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv_block(1,128),\n",
    "            conv_block(128,128),\n",
    "            conv_block(128,128),\n",
    "            conv_block(128,128)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        #Is there risk for this to be super slow?\n",
    "        #A naive approach might transform the same data more than once?\n",
    "        #Lookup tables?\n",
    "        if self.raw_transformer is not None:\n",
    "            x = self.raw_transformer.rtoi_standard(x)\n",
    "        (num_samples,seq_len,mel_bins) = x.shape\n",
    "        x = x.view(-1,1,seq_len,mel_bins)\n",
    "        x = self.encoder(x)\n",
    "        return x.view(x.size(0),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Will most likely lean heavily on the implementation of the DCASE2021 task 5 baseline implementation.\n",
    "'''\n",
    "def prototypical_loss(input, target, n_support):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    * Design choice: Handle most of pre-processing as part of the model (torchlibrosa)?\\n      May ultimately lead to simpler augmentation etc down the line. Work with raw audio as far as possible?\\n      \\n    * Make use of h5py library for storing training, validation and test sets?\\n      Still raw audio sets?\\n    \\n    * Incorporate pytorch Dataloader, seems prudent and a good design choice.\\n      read(h5py) file + Episodic sampler -> Dataloader?\\n      \\n    * Slight change of mind. Datagen and FeatureExtractor is not really worth spending time on for now.\\n      Sure they could be interfaces for a framework up the road but can do without for now since the loop\\n      will most likely be quite task dependent for now.\\n      \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    * Design choice: Handle most of pre-processing as part of the model (torchlibrosa)?\n",
    "      May ultimately lead to simpler augmentation etc down the line. Work with raw audio as far as possible?\n",
    "      \n",
    "    * Make use of h5py library for storing training, validation and test sets?\n",
    "      Still raw audio sets?\n",
    "    \n",
    "    * Incorporate pytorch Dataloader, seems prudent and a good design choice.\n",
    "      read(h5py) file + Episodic sampler -> Dataloader?\n",
    "      \n",
    "    * Slight change of mind. Datagen and FeatureExtractor is not really worth spending time on for now.\n",
    "      Sure they could be interfaces for a framework up the road but can do without for now since the loop\n",
    "      will most likely be quite task dependent for now.\n",
    "      \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Possibly take a h5 file as input and return X_train, Y_train, X_val, Y_val\n",
    "Is this an approach that we like? Is it commonly used for deep learning?\n",
    "'''\n",
    "\n",
    "class Datagen():\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Possibly work on an raw files and annotations and return/write h5 files.\n",
    "This might be clunky to include in a framework since this most likely is dataset dependent.\n",
    "Might however benfit from having an interface which is inherited by classes working on specific datasets.\n",
    "'''\n",
    "\n",
    "class FeatureExtractor(abc.ABC):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "'''\n",
    "Takes the data from the DCASE (all files one folder) and returns h5 file with the datasets 'features' and 'labels'\n",
    "GOAL: Start with training data. Extract raw segments of interest.\n",
    "Not perfect but does the job for now?\n",
    "'''\n",
    "class MyF_Ext(FeatureExtractor):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "    def extract_features(self):\n",
    "        \n",
    "        if self.config.set.train:\n",
    "            self.extract_train()\n",
    "        else:\n",
    "            self.extract_test()\n",
    "    \n",
    "    '''\n",
    "    Assumes all *.csv and *.wav files are in the same folder which path is in config.\n",
    "    Either creates spectrograms as features or raw audio segments containing events.\n",
    "    Assumes annotations as those provided in \n",
    "    '''\n",
    "    \n",
    "    def extract_train(self):\n",
    "        \n",
    "        \n",
    "        csv_files = [file for file in glob(os.path.join(self.config.path.data_train, '*.csv'))]\n",
    "        \n",
    "        if self.config.features.raw:\n",
    "            \n",
    "            events = []\n",
    "            labels = []\n",
    "            \n",
    "            for file in csv_files:\n",
    "            \n",
    "                print('Processing ' + file.replace('csv', 'wav'))\n",
    "                audio, sr = librosa.load(file.replace('csv', 'wav'), self.config.features.sr)\n",
    "                df = pd.read_csv(file, header=0, index_col=False)\n",
    "                df_pos = df[(df == 'POS').any(axis=1)]\n",
    "                \n",
    "                #Add config options for window size around event\n",
    "                df_pos.loc[:, 'Starttime'] = df_pos['Starttime'] - 0.025\n",
    "                df_pos.loc[:, 'Endtime'] = df_pos['Endtime'] + 0.025\n",
    "                start_time = [int(np.floor(start * sr)) for start in df_pos['Starttime']]\n",
    "                end_time = [int(np.floor(end * sr)) for end in df_pos['Endtime']]\n",
    "                \n",
    "                #Better way of doing this?\n",
    "                for i in range(len(start_time)):\n",
    "                    events += [audio[start_time[i]:end_time[i]]]\n",
    "                    \n",
    "                labels += list(chain.from_iterable(\n",
    "                    [df_pos.columns[(df_pos == 'POS').loc[index]].values for index, _ in df_pos.iterrows()]))\n",
    "            \n",
    "            print('Padding')\n",
    "            #Pad arrays in events and format for write\n",
    "            max_len = 0\n",
    "            for e in events:\n",
    "                if len(e) > max_len:\n",
    "                    max_len = len(e)\n",
    "                    \n",
    "            for i in range(len(events)):\n",
    "                if len(events[i]) < max_len:\n",
    "                    events[i] = np.append(events[i], np.array([self.config.features.raw_pad]*(max_len-len(events[i]))))\n",
    "            \n",
    "            events = np.array(events)\n",
    "            \n",
    "            print('Writing to file')\n",
    "            \n",
    "            hf = h5py.File(os.path.join(self.config.path.train_w, 'raw_train.h5'), 'w')\n",
    "            hf.create_dataset('features', data=events)\n",
    "            hf.create_dataset('labels', data=[s.encode() for s in labels], dtype='S20')\n",
    "            hf.close()\n",
    "            \n",
    "            print('Done')\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #DCASE more or less\n",
    "            \n",
    "            fps = self.config.features.sr / self.config.hop_mel\n",
    "            seg_len = int(round(self.config.features.seg_len * fps))\n",
    "            hop_seg = int(round(self.config.features.hop_seg * fps))\n",
    "            \n",
    "            \n",
    "            events = []\n",
    "            labels = []\n",
    "            \n",
    "            for file in csv_files:\n",
    "                \n",
    "                print('Processing ' + file.replace('csv', 'wav'))\n",
    "                audio, sr = librosa.load(file.replace('csv', 'wav'), self.config.features.sr)\n",
    "                \n",
    "                #PCEN STUFF HERE\n",
    "                \n",
    "                df = pd.read_csv(file, header=0, index_col=False)\n",
    "                df_pos = df[(df == 'POS').any(axis=1)]\n",
    "                \n",
    "                start_time, end_time = time_2_frame(df_pos, fps)\n",
    "                label_f = list(chain.from_iterable(\n",
    "                    [df_pos.columns[(df_pos == 'POS').loc[index]].values for index, _ in df_pos.iterrows()]))\n",
    "                \n",
    "                for index in range(len(start_time)):\n",
    "                    \n",
    "                    str_ind = start_time[index]\n",
    "                    end_ind = end_time[index]\n",
    "                    label = label_f[index]\n",
    "                    \n",
    "                    if end_ind - str_ind > seg_len:\n",
    "                        shift = 0\n",
    "                        while end_ind - (str_ind + shift) > seg_len:\n",
    "                            \n",
    "                \n",
    "    \n",
    "    def extract_test(self, config):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instance with torchlibrosa to be included in model if input is raw.\n",
    "\n",
    "class RawTransformer:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        #Mel stuff etc\n",
    "        self.config = config\n",
    "    \n",
    "    #Input is a training batch?\n",
    "    def rtoi_standard(input):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE 2021 ish\n",
    "#Instance given to DataLoader on argument batch_sampler\n",
    "\n",
    "class RandomEpisodicSampler(data.Sampler):\n",
    "    \n",
    "    #Include the option to choose the number of query samples\n",
    "    #Y_train -> labels, just a list of the targets (list of ints?)\n",
    "    def __init__(self, labels, n_episodes, n_way, n_support, n_query):\n",
    "        \n",
    "        #Number of episodes per epoch. len(labels)/(n_support * n_query) ?\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_way = n_way\n",
    "        self.n_support = n_support\n",
    "        self.n_query = n_query\n",
    "        \n",
    "        self.sample_indices = []\n",
    "        for i in range(max(labels) + 1):\n",
    "            ix = np.argwhere(labels == i).reshape(-1)\n",
    "            ix = torch.from_numpy(ix)\n",
    "            self.sample_indices.append(ix)\n",
    "            \n",
    "        if self.n_way > len(self.samples_indices):\n",
    "            raise ValueError('Error: \"n_way\" parameter is higher than the unique number of classes')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "    \n",
    "    def __iter__(self):\n",
    "        yield None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config/parse/util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_2_frame(df,fps):\n",
    "\n",
    "\n",
    "    'Margin of 25 ms around the onset and offsets'\n",
    "\n",
    "    df.loc[:,'Starttime'] = df['Starttime'] - 0.025\n",
    "    df.loc[:,'Endtime'] = df['Endtime'] + 0.025\n",
    "\n",
    "    'Converting time to frames'\n",
    "\n",
    "    start_time = [int(np.floor(start * fps)) for start in df['Starttime']]\n",
    "\n",
    "    end_time = [int(np.floor(end * fps)) for end in df['Endtime']]\n",
    "\n",
    "    return start_time,end_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-534a65161f8f>:1: UserWarning: config_path is not specified in hydra.initialize().\n",
      "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_hydra_main_config_path for more information.\n",
      "  initialize(job_name='test')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize(job_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing C:\\Users\\marti\\Documents\\playground\\data\\small\\2015-09-04_08-04-59_unit03.wav\n",
      "Processing C:\\Users\\marti\\Documents\\playground\\data\\small\\dcase_MK1.wav\n",
      "Padding\n",
      "Writing to file\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "cfg = compose(config_name='config')\n",
    "\n",
    "f_ext = MyF_Ext(cfg)\n",
    "f_ext.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
