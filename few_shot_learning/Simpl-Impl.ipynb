{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "How to make the framework flexible enough that one can point to which samples in a batch are meant to be\n",
    "support/query per class? The implementation in DCASE2021 does not handle this.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototypical net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE2021\n",
    "\n",
    "def conv_block(in_channels,out_channels):\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels,out_channels,3,padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE2021\n",
    "\n",
    "#TODO introduce parametrization of conv blocks?\n",
    "class Protonet(nn.Module):\n",
    "    def __init__(self, raw_transformer=None):\n",
    "        super(Protonet,self).__init__()\n",
    "        self.raw_transformer = raw_transformer\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv_block(1,128),\n",
    "            conv_block(128,128),\n",
    "            conv_block(128,128),\n",
    "            conv_block(128,128)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        #Is there risk for this to be super slow?\n",
    "        #A naive approach might transform the same data more than once?\n",
    "        #Lookup tables?\n",
    "        if self.raw_transformer is not None:\n",
    "            x = self.raw_transformer.rtoi_standard(x)\n",
    "        (num_samples,seq_len,mel_bins) = x.shape\n",
    "        x = x.view(-1,1,seq_len,mel_bins)\n",
    "        x = self.encoder(x)\n",
    "        return x.view(x.size(0),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Will most likely lean heavily on the implementation of the DCASE2021 task 5 baseline implementation.\n",
    "'''\n",
    "def prototypical_loss(input, target, n_support):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    * Design choice: Handle most of pre-processing as part of the model (torchlibrosa)?\n",
    "      May ultimately lead to simpler augmentation etc down the line. Work with raw audio as far as possible?\n",
    "      \n",
    "    * Make use of h5py library for storing training, validation and test sets?\n",
    "      Still raw audio sets?\n",
    "    \n",
    "    * Incorporate pytorch Dataloader, seems prudent and a good design choice.\n",
    "      read(h5py) file + Episodic sampler -> Dataloader?\n",
    "      \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Possibly take a h5 file as input and return X_train, Y_train, X_val, Y_val\n",
    "Is this an approach that we like? Is it commonly used for deep learning?\n",
    "'''\n",
    "\n",
    "class Datagen():\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Possibly work on an raw files and annotations and return/write h5 files.\n",
    "This might be clunky to include in a framework since this most likely is dataset dependent.\n",
    "Might however benfit from having an interface which is inherited by classes working on specific datasets.\n",
    "'''\n",
    "\n",
    "class FeatureExtractor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instance with torchlibrosa to be included in model if input is raw.\n",
    "\n",
    "class RawTransformer:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        #Mel stuff etc\n",
    "        self.config = config\n",
    "    \n",
    "    #Input is a training batch?\n",
    "    def rtoi_standard(input):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DCASE 2021 ish\n",
    "#Instance given to DataLoader on argument batch_sampler\n",
    "\n",
    "class RandomEpisodicSampler(data.Sampler):\n",
    "    \n",
    "    #Include the option to choose the number of query samples\n",
    "    #Y_train -> labels, just a list of the targets (list of ints?)\n",
    "    def __init__(self, labels, n_episodes, n_way, n_support, n_query):\n",
    "        \n",
    "        #Number of episodes per epoch. len(labels)/(n_support * n_query) ?\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_way = n_way\n",
    "        self.n_support = n_support\n",
    "        self.n_query = n_query\n",
    "        \n",
    "        self.sample_indices = []\n",
    "        for i in range(max(labels) + 1):\n",
    "            ix = np.argwhere(labels == i).reshape(-1)\n",
    "            ix = torch.from_numpy(ix)\n",
    "            self.sample_indices.append(ix)\n",
    "            \n",
    "        if self.n_way > len(self.samples_indices):\n",
    "            raise ValueError('Error: \"n_way\" parameter is higher than the unique number of classes')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "    \n",
    "    def __iter__(self):\n",
    "        yield None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config/parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
